{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"fusionlib Documentation","text":"<p>This is the fusionlib project. It is a Python package designed to provide functionality related to model fusion.</p> <p>Note</p> <ul> <li>Any questions or comments can be directed to the GitHub Issues page for this project.</li> <li>Any contributions or pull requests are welcome.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>This project is currently under development. For the latest features and bug fixes, it is recommended to install directly from the GitHub repository. To install the latest development version from GitHub, use the following command:</p> <pre><code>git clone https://github.com/tanganke/fusionlib.git\npip3 install -e fusionlib\n</code></pre> <p>or install from PyPI:</p> <p>Install from PyPI</p> <p>This project is currently under development. For the latest features and bug fixes, it is recommended to install directly from the GitHub repository.</p> <pre><code>pip3 install -U fusionlib\n</code></pre>"},{"location":"api/fusionlib/","title":"fusionlib","text":""},{"location":"api/fusionlib/#fusionlib","title":"<code>fusionlib</code>","text":""},{"location":"api/fusionlib/#fusionlib.merge","title":"<code>fusionlib.merge</code>","text":""},{"location":"api/fusionlib/#fusionlib.merge.adamerging","title":"<code>fusionlib.merge.adamerging</code>","text":""},{"location":"api/fusionlib/#fusionlib.merge.adamerging.ModelWrapper","title":"<code>fusionlib.merge.adamerging.ModelWrapper</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>fusionlib/merge/adamerging.py</code> <pre><code>class ModelWrapper(torch.nn.Module):\n    def __init__(self, model, initial_weights=None):\n        \"\"\"\n        Initializes a wrapper for a PyTorch model.\n\n        Args:\n            model (nn.Module): PyTorch model to wrap.\n            initial_weights (optional): Initial weights for the model. Defaults to None.\n        \"\"\"\n        super(ModelWrapper, self).__init__()\n        self.model = model\n\n        if hasattr(self.model, \"transformer\"):\n            delattr(self.model, \"transformer\")\n\n    def forward(self, images: Tensor) -&gt; Tensor:\n        \"\"\"\n        Forward pass of the wrapped PyTorch model.\n\n        Args:\n            images (Tensor): Input tensor.\n\n        Returns:\n            Tensor: Output tensor.\n        \"\"\"\n        features = self.model(images)\n        return features\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.merge.adamerging.ModelWrapper.__init__","title":"<code>fusionlib.merge.adamerging.ModelWrapper.__init__(model, initial_weights=None)</code>","text":"<p>Initializes a wrapper for a PyTorch model.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>PyTorch model to wrap.</p> </li> <li> <code>initial_weights</code>               (<code>optional</code>, default:                   <code>None</code> )           \u2013            <p>Initial weights for the model. Defaults to None.</p> </li> </ul> Source code in <code>fusionlib/merge/adamerging.py</code> <pre><code>def __init__(self, model, initial_weights=None):\n    \"\"\"\n    Initializes a wrapper for a PyTorch model.\n\n    Args:\n        model (nn.Module): PyTorch model to wrap.\n        initial_weights (optional): Initial weights for the model. Defaults to None.\n    \"\"\"\n    super(ModelWrapper, self).__init__()\n    self.model = model\n\n    if hasattr(self.model, \"transformer\"):\n        delattr(self.model, \"transformer\")\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.merge.adamerging.ModelWrapper.forward","title":"<code>fusionlib.merge.adamerging.ModelWrapper.forward(images)</code>","text":"<p>Forward pass of the wrapped PyTorch model.</p> <p>Parameters:</p> <ul> <li> <code>images</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor.</p> </li> </ul> Source code in <code>fusionlib/merge/adamerging.py</code> <pre><code>def forward(self, images: Tensor) -&gt; Tensor:\n    \"\"\"\n    Forward pass of the wrapped PyTorch model.\n\n    Args:\n        images (Tensor): Input tensor.\n\n    Returns:\n        Tensor: Output tensor.\n    \"\"\"\n    features = self.model(images)\n    return features\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.merge.adamerging.del_attr","title":"<code>fusionlib.merge.adamerging.del_attr(obj, names)</code>","text":"<p>Deletes an attribute from an object recursively.</p> <p>Parameters:</p> <ul> <li> <code>obj</code>               (<code>object</code>)           \u2013            <p>Object to delete attribute from.</p> </li> <li> <code>names</code>               (<code>list</code>)           \u2013            <p>List of attribute names to delete recursively.</p> </li> </ul> Source code in <code>fusionlib/merge/adamerging.py</code> <pre><code>def del_attr(obj, names: List[str]):\n    \"\"\"\n    Deletes an attribute from an object recursively.\n\n    Args:\n        obj (object): Object to delete attribute from.\n        names (list): List of attribute names to delete recursively.\n    \"\"\"\n    if len(names) == 1:\n        delattr(obj, names[0])\n    else:\n        del_attr(getattr(obj, names[0]), names[1:])\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.merge.adamerging.load_weights","title":"<code>fusionlib.merge.adamerging.load_weights(mod, names, params)</code>","text":"<p>Loads weights into a PyTorch module.</p> <p>Parameters:</p> <ul> <li> <code>mod</code>               (<code>Module</code>)           \u2013            <p>PyTorch module to load weights into.</p> </li> <li> <code>names</code>               (<code>list</code>)           \u2013            <p>List of parameter names to load weights into.</p> </li> <li> <code>params</code>               (<code>tuple</code>)           \u2013            <p>Tuple of weights to load into the module.</p> </li> </ul> Source code in <code>fusionlib/merge/adamerging.py</code> <pre><code>def load_weights(mod, names, params):\n    \"\"\"\n    Loads weights into a PyTorch module.\n\n    Args:\n        mod (nn.Module): PyTorch module to load weights into.\n        names (list): List of parameter names to load weights into.\n        params (tuple): Tuple of weights to load into the module.\n    \"\"\"\n    for name, p in zip(names, params):\n        set_attr(mod, name.split(\".\"), p)\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.merge.adamerging.make_functional","title":"<code>fusionlib.merge.adamerging.make_functional(mod)</code>","text":"<p>Converts a PyTorch module into a functional module by removing all parameters and returning their names.</p> <p>Parameters:</p> <ul> <li> <code>mod</code>               (<code>Module</code>)           \u2013            <p>PyTorch module to be converted.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Tuple[Tensor]: Tuple containing the original parameters of the module.</p> </li> <li>           \u2013            <p>List[str]: List containing the names of the removed parameters.</p> </li> </ul> Source code in <code>fusionlib/merge/adamerging.py</code> <pre><code>def make_functional(mod: nn.Module):\n    \"\"\"\n    Converts a PyTorch module into a functional module by removing all parameters and returning their names.\n\n    Args:\n        mod (nn.Module): PyTorch module to be converted.\n\n    Returns:\n        Tuple[Tensor]: Tuple containing the original parameters of the module.\n        List[str]: List containing the names of the removed parameters.\n    \"\"\"\n    orig_params = tuple(mod.parameters())\n    names: List[str] = []\n    for name, p in list(mod.named_parameters()):\n        del_attr(mod, name.split(\".\"))\n        names.append(name)\n    return orig_params, names\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.merge.adamerging.set_attr","title":"<code>fusionlib.merge.adamerging.set_attr(obj, names, val)</code>","text":"<p>Sets an attribute of an object recursively.</p> <p>Parameters:</p> <ul> <li> <code>obj</code>               (<code>object</code>)           \u2013            <p>Object to set attribute of.</p> </li> <li> <code>names</code>               (<code>list</code>)           \u2013            <p>List of attribute names to set recursively.</p> </li> <li> <code>val</code>               (<code>object</code>)           \u2013            <p>Value to set the attribute to.</p> </li> </ul> Source code in <code>fusionlib/merge/adamerging.py</code> <pre><code>def set_attr(obj, names: List[str], val):\n    \"\"\"\n    Sets an attribute of an object recursively.\n\n    Args:\n        obj (object): Object to set attribute of.\n        names (list): List of attribute names to set recursively.\n        val (object): Value to set the attribute to.\n    \"\"\"\n    if len(names) == 1:\n        setattr(obj, names[0], val)\n    else:\n        set_attr(getattr(obj, names[0]), names[1:], val)\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.merge.adamerging.softmax_entropy","title":"<code>fusionlib.merge.adamerging.softmax_entropy(x)</code>","text":"<p>Computes the softmax entropy of a tensor.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>          \u2013            <p>Softmax entropy of the input tensor.</p> </li> </ul> Source code in <code>fusionlib/merge/adamerging.py</code> <pre><code>def softmax_entropy(x: Tensor):\n    \"\"\"\n    Computes the softmax entropy of a tensor.\n\n    Args:\n        x (Tensor): Input tensor.\n\n    Returns:\n        Tensor: Softmax entropy of the input tensor.\n    \"\"\"\n    probs = x.softmax(-1)\n    return -(probs * probs.log()).sum(-1)\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.merge.average","title":"<code>fusionlib.merge.average</code>","text":""},{"location":"api/fusionlib/#fusionlib.merge.average.simple_average","title":"<code>fusionlib.merge.average.simple_average(modules)</code>","text":"<p>Averages the parameters of a list of PyTorch modules or state dictionaries.</p> <p>This function takes a list of PyTorch modules or state dictionaries and returns a new module with the averaged parameters, or a new state dictionary with the averaged parameters.</p> <p>Parameters:</p> <ul> <li> <code>modules</code>               (<code>List[Union[Module, _StateDict]]</code>)           \u2013            <p>A list of PyTorch modules or state dictionaries.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>module_or_state_dict</code> (              <code>Union[Module, _StateDict]</code> )          \u2013            <p>A new PyTorch module with the averaged parameters, or a new state dictionary with the averaged parameters.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch.nn as nn\n&gt;&gt;&gt; model1 = nn.Linear(10, 10)\n&gt;&gt;&gt; model2 = nn.Linear(10, 10)\n&gt;&gt;&gt; averaged_model = simple_averageing([model1, model2])\n</code></pre> <pre><code>&gt;&gt;&gt; state_dict1 = model1.state_dict()\n&gt;&gt;&gt; state_dict2 = model2.state_dict()\n&gt;&gt;&gt; averaged_state_dict = simple_averageing([state_dict1, state_dict2])\n</code></pre> Source code in <code>fusionlib/merge/average.py</code> <pre><code>def simple_average(modules: List[Union[nn.Module, _StateDict]]):\n    \"\"\"\n    Averages the parameters of a list of PyTorch modules or state dictionaries.\n\n    This function takes a list of PyTorch modules or state dictionaries and returns a new module with the averaged parameters, or a new state dictionary with the averaged parameters.\n\n    Args:\n        modules (List[Union[nn.Module, _StateDict]]): A list of PyTorch modules or state dictionaries.\n\n    Returns:\n        module_or_state_dict (Union[nn.Module, _StateDict]): A new PyTorch module with the averaged parameters, or a new state dictionary with the averaged parameters.\n\n    Examples:\n        &gt;&gt;&gt; import torch.nn as nn\n        &gt;&gt;&gt; model1 = nn.Linear(10, 10)\n        &gt;&gt;&gt; model2 = nn.Linear(10, 10)\n        &gt;&gt;&gt; averaged_model = simple_averageing([model1, model2])\n\n        &gt;&gt;&gt; state_dict1 = model1.state_dict()\n        &gt;&gt;&gt; state_dict2 = model2.state_dict()\n        &gt;&gt;&gt; averaged_state_dict = simple_averageing([state_dict1, state_dict2])\n    \"\"\"\n    if isinstance(modules[0], nn.Module):\n        new_module = deepcopy(modules[0])\n        state_dict = state_dict_avg([module.state_dict() for module in modules])\n        new_module.load_state_dict(state_dict)\n        return new_module\n    elif isinstance(modules[0], Mapping):\n        return state_dict_avg(modules)\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.merge.concrete_mask","title":"<code>fusionlib.merge.concrete_mask</code>","text":""},{"location":"api/fusionlib/#fusionlib.merge.concrete_mask.ConcreteMask","title":"<code>fusionlib.merge.concrete_mask.ConcreteMask</code>","text":"<p>               Bases: <code>Module</code></p> <p>This class represents a ConcreteMask, which is a type of mask that can be applied to a state dictionary / task vector. It is used to create a mask for each parameter in the state dictionary and apply it to the state dictionary.</p> <p>Attributes:</p> <ul> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>The temperature parameter for the RelaxedBernoulli distribution.</p> </li> <li> <code>masks</code>               (<code>ParameterDict</code>)           \u2013            <p>A dictionary of masks for each parameter in the state dictionary.</p> </li> </ul> Source code in <code>fusionlib/merge/concrete_mask.py</code> <pre><code>class ConcreteMask(nn.Module):\n    \"\"\"\n    This class represents a ConcreteMask, which is a type of mask that can be applied to a state dictionary / task vector.\n    It is used to create a mask for each parameter in the state dictionary and apply it to the state dictionary.\n\n    Attributes:\n        temperature (float): The temperature parameter for the RelaxedBernoulli distribution.\n        masks (nn.ParameterDict): A dictionary of masks for each parameter in the state dictionary.\n    \"\"\"\n\n    def __init__(\n        self,\n        temperature: float,\n        state_dict: _StateDict,\n        init_value: float = 5.0,\n        draw_sample: bool = True,\n    ):\n        super().__init__()\n        self.temperature = temperature\n        masks = {}\n        for k, v in state_dict.items():\n            masks[k] = nn.Parameter(torch.ones_like(v) * init_value, requires_grad=True)\n            init_device = v.device\n        self.masks = masks\n        self.draw_sample = draw_sample\n\n    def _draw_mask(self, binary_mask: Optional[bool] = False):\n        \"\"\"\n        Draws a mask based on the current state of the object.\n\n        This function uses a relaxed Bernoulli distribution to draw a mask. If `binary_mask` is True,\n        the function will return a binary mask. Otherwise, it will return a mask based on the probabilities\n        from the distribution.\n\n        Parameters:\n            binary_mask (bool, optional): If True, the function will return a binary mask. Defaults to False.\n\n        Returns:\n            dict: A dictionary where the keys are the same as the keys in `self.masks` and the values are the drawn masks.\n        \"\"\"\n        concrete_masks = {}\n        for k in self.masks.keys():\n            concrete_dist = torch.distributions.RelaxedBernoulli(\n                self.temperature,\n                logits=self.masks[k],\n            )\n            if binary_mask == True:\n                concrete_mask: Tensor = (concrete_dist.sample()).detach_() &gt; 0.5\n            else:\n                if self.draw_sample:\n                    # this is slow on cpu\n                    concrete_mask = concrete_dist.rsample()\n                else:\n                    concrete_mask = concrete_dist.probs\n            concrete_masks[k] = concrete_mask\n        return concrete_masks\n\n    def _apply_mask(self, concrete_masks, state_dict: _StateDict):\n        \"\"\"\n        This method applies the mask to the state dictionary and rescale it.\n\n        Args:\n            concrete_masks (StateDict): The concrete masks to be applied.\n            state_dict (StateDict): The state dictionary to which the mask will be applied.\n\n        Returns:\n            StateDict: The state dictionary after the mask has been applied.\n        \"\"\"\n        new_state_dict = {}\n        for k, v in state_dict.items():\n            concrete_mask = concrete_masks[k]\n            new_state_dict[k] = v * concrete_mask / concrete_mask.mean()\n        return new_state_dict\n\n    def apply_mask(\n        self, state_dicts: List[_StateDict], concrete_masks: Optional[_StateDict] = None\n    ):\n        \"\"\"\n        This method applies the mask to the state dictionary and rescales it.\n\n        Args:\n            state_dict (StateDict): The state dictionary to which the mask will be applied.\n\n        Returns:\n            StateDict: The state dictionary after the mask has been applied and rescaled.\n        \"\"\"\n        # draw common mask\n        if concrete_masks is None:\n            concrete_masks = self._draw_mask()\n\n        _mask_on_device = {}\n\n        def mask_on_device(device: torch.device):\n            if device in _mask_on_device:\n                return _mask_on_device[device]\n            else:\n                _mask_on_device[device] = {\n                    k: v.to(device, non_blocking=True)\n                    for k, v in concrete_masks.items()\n                }\n                return _mask_on_device[device]\n\n        # mask and rescale\n        new_state_dicts = []\n        for state_dict in state_dicts:\n            device = next(iter(state_dict.values())).device\n            new_state_dict = self._apply_mask(mask_on_device(device), state_dict)\n            new_state_dicts.append(new_state_dict)\n        return new_state_dicts\n\n    def parameters(self, recurse: bool = True) -&gt; Iterator[Parameter]:\n        return self.masks.values()\n\n    def to(self, device):\n        for k in self.masks.keys():\n            self.masks[k] = self.masks[k].to(device)\n        return self\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.merge.concrete_mask.ConcreteMask.apply_mask","title":"<code>fusionlib.merge.concrete_mask.ConcreteMask.apply_mask(state_dicts, concrete_masks=None)</code>","text":"<p>This method applies the mask to the state dictionary and rescales it.</p> <p>Parameters:</p> <ul> <li> <code>state_dict</code>               (<code>StateDict</code>)           \u2013            <p>The state dictionary to which the mask will be applied.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>StateDict</code>          \u2013            <p>The state dictionary after the mask has been applied and rescaled.</p> </li> </ul> Source code in <code>fusionlib/merge/concrete_mask.py</code> <pre><code>def apply_mask(\n    self, state_dicts: List[_StateDict], concrete_masks: Optional[_StateDict] = None\n):\n    \"\"\"\n    This method applies the mask to the state dictionary and rescales it.\n\n    Args:\n        state_dict (StateDict): The state dictionary to which the mask will be applied.\n\n    Returns:\n        StateDict: The state dictionary after the mask has been applied and rescaled.\n    \"\"\"\n    # draw common mask\n    if concrete_masks is None:\n        concrete_masks = self._draw_mask()\n\n    _mask_on_device = {}\n\n    def mask_on_device(device: torch.device):\n        if device in _mask_on_device:\n            return _mask_on_device[device]\n        else:\n            _mask_on_device[device] = {\n                k: v.to(device, non_blocking=True)\n                for k, v in concrete_masks.items()\n            }\n            return _mask_on_device[device]\n\n    # mask and rescale\n    new_state_dicts = []\n    for state_dict in state_dicts:\n        device = next(iter(state_dict.values())).device\n        new_state_dict = self._apply_mask(mask_on_device(device), state_dict)\n        new_state_dicts.append(new_state_dict)\n    return new_state_dicts\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.merge.task_arithmetic","title":"<code>fusionlib.merge.task_arithmetic</code>","text":""},{"location":"api/fusionlib/#fusionlib.merge.task_arithmetic.task_arithmetic_merge_modules","title":"<code>fusionlib.merge.task_arithmetic.task_arithmetic_merge_modules(pretrained_model, finetuned_models, scaling_coef)</code>","text":"<p>Merges a pretrained model with a list of fine-tuned models using task arithmetic.</p> <p>Parameters:</p> <ul> <li> <code>pretrained_model</code>               (<code>Module</code>)           \u2013            <p>The pretrained model.</p> </li> <li> <code>finetuned_models</code>               (<code>List[Module]</code>)           \u2013            <p>A list of fine-tuned models.</p> </li> <li> <code>scaling_coef</code>               (<code>float</code>)           \u2013            <p>The scaling coefficient to apply to the sum of the fine-tuned state dictionaries.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>nn.Module: The merged model.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pretrained_model = torch.nn.Linear(10, 10)\n&gt;&gt;&gt; finetuned_models = [torch.nn.Linear(10, 10), torch.nn.Linear(10, 10)]\n&gt;&gt;&gt; scaling_coef = 0.1\n&gt;&gt;&gt; new_model = task_arithmetic_merge_modules(pretrained_model, finetuned_models, scaling_coef)\n</code></pre> Source code in <code>fusionlib/merge/task_arithmetic.py</code> <pre><code>@torch.no_grad()\ndef task_arithmetic_merge_modules(\n    pretrained_model: nn.Module, finetuned_models: List[nn.Module], scaling_coef: float\n):\n    \"\"\"\n    Merges a pretrained model with a list of fine-tuned models using task arithmetic.\n\n    Args:\n        pretrained_model (nn.Module): The pretrained model.\n        finetuned_models (List[nn.Module]): A list of fine-tuned models.\n        scaling_coef (float): The scaling coefficient to apply to the sum of the fine-tuned state dictionaries.\n\n    Returns:\n        nn.Module: The merged model.\n\n    Examples:\n        &gt;&gt;&gt; pretrained_model = torch.nn.Linear(10, 10)\n        &gt;&gt;&gt; finetuned_models = [torch.nn.Linear(10, 10), torch.nn.Linear(10, 10)]\n        &gt;&gt;&gt; scaling_coef = 0.1\n        &gt;&gt;&gt; new_model = task_arithmetic_merge_modules(pretrained_model, finetuned_models, scaling_coef)\n    \"\"\"\n    pretrained_state_dict = pretrained_model.state_dict()\n    finetuned_state_dicts = [model.state_dict() for model in finetuned_models]\n    new_state_dict = task_arithmetic_merge_state_dicts(\n        pretrained_state_dict, finetuned_state_dicts, scaling_coef\n    )\n    model = deepcopy(pretrained_model)\n    model.load_state_dict(new_state_dict)\n    return model\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.merge.task_arithmetic.task_arithmetic_merge_state_dicts","title":"<code>fusionlib.merge.task_arithmetic.task_arithmetic_merge_state_dicts(pretrained_state_dict, finetuned_state_dicts, scaling_coef)</code>","text":"<p>Examples:</p> <pre><code>&gt;&gt;&gt; pretrained_state_dict = model.state_dict()\n&gt;&gt;&gt; finetuned_state_dicts = [model1.state_dict(), model2.state_dict()]\n&gt;&gt;&gt; scaling_coef = 0.1\n&gt;&gt;&gt; new_state_dict = task_arithmetic_merge_state_dicts(pretrained_state_dict, finetuned_state_dicts, scaling_coef)\n</code></pre> Source code in <code>fusionlib/merge/task_arithmetic.py</code> <pre><code>def task_arithmetic_merge_state_dicts(\n    pretrained_state_dict: _StateDict,\n    finetuned_state_dicts: List[_StateDict],\n    scaling_coef: float,\n):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; pretrained_state_dict = model.state_dict()\n        &gt;&gt;&gt; finetuned_state_dicts = [model1.state_dict(), model2.state_dict()]\n        &gt;&gt;&gt; scaling_coef = 0.1\n        &gt;&gt;&gt; new_state_dict = task_arithmetic_merge_state_dicts(pretrained_state_dict, finetuned_state_dicts, scaling_coef)\n    \"\"\"\n    task_vectors = [\n        state_dict_sub(finetuned_state_dict, pretrained_state_dict)\n        for finetuned_state_dict in finetuned_state_dicts\n    ]\n    task_vector = state_dict_sum(task_vectors)\n    task_vector = state_dict_mul(task_vector, scaling_coef)\n    new_state_dict = state_dict_add(pretrained_state_dict, task_vector)\n    return new_state_dict\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.merge.wrapper","title":"<code>fusionlib.merge.wrapper</code>","text":""},{"location":"api/fusionlib/#fusionlib.merge.wrapper.block_wise_fusion","title":"<code>fusionlib.merge.wrapper.block_wise_fusion</code>","text":"<p>This module provides functions for fusing the weights of multiple deep neural networks block-wisely.</p>"},{"location":"api/fusionlib/#fusionlib.merge.wrapper.block_wise_fusion.BlockWiseMergedModel","title":"<code>fusionlib.merge.wrapper.block_wise_fusion.BlockWiseMergedModel</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>fusionlib/merge/wrapper/block_wise_fusion.py</code> <pre><code>class BlockWiseMergedModel(nn.Module):\n    def __init__(\n        self,\n        pretrained_model: nn.Module,\n        block_wise_weight: Dict[str, Tensor],\n        task_vectors: List[_StateDict],\n        clamp_weights: bool = True,\n    ):\n        super().__init__()\n        self.model = deepcopy(pretrained_model)\n        for p in self.model.parameters():\n            p.requires_grad = False\n\n        self.block_wise_weight = {\n            k: nn.Parameter(v, requires_grad=True) for k, v in block_wise_weight.items()\n        }\n        self.task_vectors = task_vectors\n        self.pretrained_state_vector = self.model.state_dict()\n        check_parameters_all_equal([self.pretrained_state_vector] + self.task_vectors)\n        self.clamp_weights = clamp_weights\n        self._state_dict = None\n\n    def parameters(self, recurse: bool = True) -&gt; Iterator[nn.Parameter]:\n        return iter(self.block_wise_weight.values())\n\n    def _get_merged_vector(self):\n        \"\"\"\n        Returns the fused weights of the model by applying the block-wise fusion algorithm.\n\n        Returns:\n            The fused weights of the model.\n        \"\"\"\n        if self.clamp_weights:\n            block_wise_weight = {\n                k: torch.clamp(p, min=0, max=1)\n                for k, p in self.block_wise_weight.items()\n            }\n        else:\n            block_wise_weight = self.block_wise_weight\n        return fuse_weights(block_wise_weight, self.task_vectors)\n\n    def merge_weights(self):\n        \"\"\"\n        Merges the weights of the model.\n        Call this after each update step.\n        \"\"\"\n        with timer(loglevel=logging.DEBUG):\n            log.debug(\"Merging weights\")\n            task_vector = self._get_merged_vector()\n            self._state_dict = {\n                k: (task_vector[k] + self.pretrained_state_vector[k]).cuda(\n                    non_blocking=True\n                )\n                for k in self.pretrained_state_vector.keys()\n            }\n\n    def forward(self, *args, **kwargs):\n        if self._state_dict is None:\n            self.merge_weights()\n        return functional_call(\n            self.model,\n            self._state_dict,\n            args=args,\n            kwargs=kwargs,\n        )\n</code></pre> <code></code> <code>fusionlib.merge.wrapper.block_wise_fusion.BlockWiseMergedModel.merge_weights()</code> \u00b6 <p>Merges the weights of the model. Call this after each update step.</p> Source code in <code>fusionlib/merge/wrapper/block_wise_fusion.py</code> <pre><code>def merge_weights(self):\n    \"\"\"\n    Merges the weights of the model.\n    Call this after each update step.\n    \"\"\"\n    with timer(loglevel=logging.DEBUG):\n        log.debug(\"Merging weights\")\n        task_vector = self._get_merged_vector()\n        self._state_dict = {\n            k: (task_vector[k] + self.pretrained_state_vector[k]).cuda(\n                non_blocking=True\n            )\n            for k in self.pretrained_state_vector.keys()\n        }\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.merge.wrapper.block_wise_fusion.fuse_weights","title":"<code>fusionlib.merge.wrapper.block_wise_fusion.fuse_weights(block_wise_weight, state_dicts)</code>","text":"<p>Returns a state dict of fused weights for a given list of state dicts of weights.</p> <p>Parameters:</p> <ul> <li> <code>block_wise_weight</code>               (<code>Dict[str, Tensor]</code>)           \u2013            <p>The dictionary of tensors of block-wise weights to fuse.</p> </li> <li> <code>state_dicts</code>               (<code>List[STATE_DICT]</code>)           \u2013            <p>The list of state dicts of weights to fuse.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>STATE_DICT</code>          \u2013            <p>A state dict of fused weights.</p> </li> </ul> Source code in <code>fusionlib/merge/wrapper/block_wise_fusion.py</code> <pre><code>def fuse_weights(block_wise_weight: Dict[str, Tensor], state_dicts: List[_StateDict]):\n    \"\"\"\n    Returns a state dict of fused weights for a given list of state dicts of weights.\n\n    Args:\n        block_wise_weight (Dict[str, Tensor]): The dictionary of tensors of block-wise weights to fuse.\n        state_dicts (List[STATE_DICT]): The list of state dicts of weights to fuse.\n\n    Returns:\n        STATE_DICT: A state dict of fused weights.\n    \"\"\"\n    assert isinstance(\n        state_dicts, list\n    ), f\"state_dicts must be a list, but got {type(state_dicts)}\"\n    check_parameters_all_equal([block_wise_weight] + state_dicts)\n    return {\n        k: _fuse_weights(\n            block_wise_weight[k], [state_dict[k] for state_dict in state_dicts]\n        )\n        for k in state_dicts[0]\n    }\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.merge.wrapper.block_wise_fusion.get_block_wise_weights_by_block_size","title":"<code>fusionlib.merge.wrapper.block_wise_fusion.get_block_wise_weights_by_block_size(params, block_size, num_models, init_value=None)</code>","text":"<p>Returns a list or dictionary of block-wise weights for a given list or dictionary of tensors of weights.</p> <p>Parameters:</p> <ul> <li> <code>params</code>               (<code>List[Tensor] | Dict[str, Tensor]</code>)           \u2013            <p>The list or dictionary of tensors of weights to compute block-wise weights for.</p> </li> <li> <code>block_size</code>               (<code>int</code>)           \u2013            <p>The size of each block.</p> </li> <li> <code>num_models</code>               (<code>int</code>)           \u2013            <p>The number of models to compute block-wise weights for.</p> </li> <li> <code>init_value</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>The initial value of the block-wise weights. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>List[Tensor] | Dict[str, Tensor]: A list or dictionary of tensors of block-wise weights with the same length or keys as the input list or dictionary if the weights tensors can be evenly split,</p> </li> <li>           \u2013            <p>otherwise a list or dictionary of tensors of size 1 with a value of 1.0.</p> </li> </ul> Source code in <code>fusionlib/merge/wrapper/block_wise_fusion.py</code> <pre><code>def get_block_wise_weights_by_block_size(\n    params: List[Tensor] | Dict[str, Tensor],\n    block_size: int,\n    num_models: int,\n    init_value: float = None,\n):\n    \"\"\"\n    Returns a list or dictionary of block-wise weights for a given list or dictionary of tensors of weights.\n\n    Args:\n        params (List[Tensor] | Dict[str, Tensor]): The list or dictionary of tensors of weights to compute block-wise weights for.\n        block_size (int): The size of each block.\n        num_models (int): The number of models to compute block-wise weights for.\n        init_value (float, optional): The initial value of the block-wise weights. Defaults to None.\n\n    Returns:\n        List[Tensor] | Dict[str, Tensor]: A list or dictionary of tensors of block-wise weights with the same length or keys as the input list or dictionary if the weights tensors can be evenly split,\n        otherwise a list or dictionary of tensors of size 1 with a value of 1.0.\n    \"\"\"\n    if isinstance(params, list):\n        return [\n            _get_block_wise_weights_by_block_size(\n                p, block_size, num_models, init_value=init_value\n            )\n            for p in params\n        ]\n    elif isinstance(params, dict):\n        return {\n            k: _get_block_wise_weights_by_block_size(\n                p, block_size, num_models, init_value=init_value\n            )\n            for k, p in params.items()\n        }\n    else:\n        raise TypeError(f\"params must be a list or a dict, but got {type(params)}\")\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.merge.wrapper.block_wise_fusion.get_block_wise_weights_by_partitions","title":"<code>fusionlib.merge.wrapper.block_wise_fusion.get_block_wise_weights_by_partitions(params, num_partitions, num_models)</code>","text":"<p>Returns a list or dictionary of block-wise weights for a given list or dictionary of tensors of weights.</p> <p>Parameters:</p> <ul> <li> <code>params</code>               (<code>List[Tensor] | Dict[str, Tensor]</code>)           \u2013            <p>The list or dictionary of tensors of weights to compute block-wise weights for.</p> </li> <li> <code>num_partitions</code>               (<code>int</code>)           \u2013            <p>The number of partitions to split the tensors into.</p> </li> <li> <code>num_models</code>               (<code>int</code>)           \u2013            <p>The number of models to compute block-wise weights for.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>List[Tensor] | Dict[str, Tensor]: A list or dictionary of tensors of block-wise weights with the same length or keys as the input list or dictionary if the weights tensors can be evenly split,</p> </li> <li>           \u2013            <p>otherwise a list or dictionary of tensors of size 1 with a value of 1.0.</p> </li> </ul> Source code in <code>fusionlib/merge/wrapper/block_wise_fusion.py</code> <pre><code>def get_block_wise_weights_by_partitions(\n    params: List[Tensor] | Dict[str, Tensor], num_partitions: int, num_models: int\n):\n    \"\"\"\n    Returns a list or dictionary of block-wise weights for a given list or dictionary of tensors of weights.\n\n    Args:\n        params (List[Tensor] | Dict[str, Tensor]): The list or dictionary of tensors of weights to compute block-wise weights for.\n        num_partitions (int): The number of partitions to split the tensors into.\n        num_models (int): The number of models to compute block-wise weights for.\n\n    Returns:\n        List[Tensor] | Dict[str, Tensor]: A list or dictionary of tensors of block-wise weights with the same length or keys as the input list or dictionary if the weights tensors can be evenly split,\n        otherwise a list or dictionary of tensors of size 1 with a value of 1.0.\n    \"\"\"\n    if isinstance(params, list):\n        return [\n            _get_block_wise_weights_by_partitions(p, num_partitions, num_models)\n            for p in params\n        ]\n    elif isinstance(params, dict):\n        return {\n            k: _get_block_wise_weights_by_partitions(p, num_partitions, num_models)\n            for k, p in params.items()\n        }\n    else:\n        raise TypeError(f\"params must be a list or a dict, but got {type(params)}\")\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.merge.wrapper.block_wise_fusion.num_partitions_per_model","title":"<code>fusionlib.merge.wrapper.block_wise_fusion.num_partitions_per_model(block_wise_weight)</code>","text":"<p>Returns the number of partitions per model for a given dictionary of tensors of block-wise weights.</p> <p>Parameters:</p> <ul> <li> <code>block_wise_weight</code>               (<code>Dict[str, Tensor]</code>)           \u2013            <p>The dictionary of tensors of block-wise weights.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>          \u2013            <p>The number of partitions per model.</p> </li> </ul> Source code in <code>fusionlib/merge/wrapper/block_wise_fusion.py</code> <pre><code>def num_partitions_per_model(block_wise_weight: Dict[str, Tensor]):\n    \"\"\"\n    Returns the number of partitions per model for a given dictionary of tensors of block-wise weights.\n\n    Args:\n        block_wise_weight (Dict[str, Tensor]): The dictionary of tensors of block-wise weights.\n\n    Returns:\n        int: The number of partitions per model.\n    \"\"\"\n    assert isinstance(\n        block_wise_weight, dict\n    ), f\"block_wise_weight must be a dict, but got {type(block_wise_weight)}\"\n    total_num_partitions = 0\n    for k, weight in block_wise_weight.items():\n        assert (\n            weight.dim() == 2\n        ), f\"block_wise_weight[{k}] must be a 2D tensor, but got {weight.dim()}D\"\n        total_num_partitions += weight.size(1)\n    return total_num_partitions\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.merge.wrapper.layer_wise_fusion","title":"<code>fusionlib.merge.wrapper.layer_wise_fusion</code>","text":""},{"location":"api/fusionlib/#fusionlib.merge.wrapper.layer_wise_fusion.LayerWiseMergedModel","title":"<code>fusionlib.merge.wrapper.layer_wise_fusion.LayerWiseMergedModel</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>fusionlib/merge/wrapper/layer_wise_fusion.py</code> <pre><code>class LayerWiseMergedModel(nn.Module):\n    def __init__(\n        self,\n        pretrained_model: nn.Module,\n        layer_wise_weights: Tensor,\n        task_vectors: List[_StateDict],\n        clamp_weights: bool = True,\n    ):\n        super().__init__()\n        self._model = (pretrained_model,)\n        for p in self.model.parameters():\n            p.requires_grad_(False)\n\n        self.layer_wise_weights = nn.Parameter(layer_wise_weights, requires_grad=True)\n        self.task_vectors = task_vectors\n        self.pretrained_state_dict = self.model.state_dict(keep_vars=True)\n        check_parameters_all_equal(self.task_vectors)\n        self.merged_state_dict = None\n        self.clamp_weights = clamp_weights\n\n    @property\n    def model(self):\n        return self._model[0]\n\n    def state_dict(self):\n        sd = self.model.state_dict()\n        if self.merged_state_dict is None:\n            self.merge_weights()\n        for k, v in self.merged_state_dict.items():\n            sd[k] = v.detach()\n        return sd\n\n    def merge_weights(self, remove_keys: List[str] = None):\n        \"\"\"\n        Merges the weights of the model.\n        Call this after each update step.\n        \"\"\"\n        if self.clamp_weights:\n            layer_wise_weights = self.layer_wise_weights.clamp(0, 1)\n        else:\n            layer_wise_weights = self.layer_wise_weights\n        device = layer_wise_weights.device\n        task_vector = fuse_weights(layer_wise_weights, self.task_vectors)\n        self.merged_state_dict = {\n            k: self.pretrained_state_dict[k].to(device, non_blocking=True)\n            for k in self.pretrained_state_dict.keys()\n        }\n        for k in task_vector.keys():\n            self.merged_state_dict[k] = self.merged_state_dict[k] + task_vector[k]\n        if remove_keys is not None:\n            for k in remove_keys:\n                self.merged_state_dict.pop(k)\n\n    def forward(self, *args, **kwargs):\n        if self.merged_state_dict is None:\n            self.merge_weights()\n        return functional_call(\n            self.model,\n            self.merged_state_dict,\n            args=args,\n            kwargs=kwargs,\n            tie_weights=False,\n        )\n\n    def __getattr__(self, name: str) -&gt; Any:\n        try:\n            return super().__getattr__(name)\n        except AttributeError:\n            attr = getattr(self.model, name)\n            if isinstance(attr, Callable):\n                warnings.warn(\n                    f\"forwarding `{name}` to the underlying model\", UserWarning\n                )\n            return attr\n\n    def __setattr__(self, name: str, value: Any) -&gt; None:\n        try:\n            super().__setattr__(name, value)\n        except AttributeError:\n            setattr(self.model, name, value)\n</code></pre> <code></code> <code>fusionlib.merge.wrapper.layer_wise_fusion.LayerWiseMergedModel.merge_weights(remove_keys=None)</code> \u00b6 <p>Merges the weights of the model. Call this after each update step.</p> Source code in <code>fusionlib/merge/wrapper/layer_wise_fusion.py</code> <pre><code>def merge_weights(self, remove_keys: List[str] = None):\n    \"\"\"\n    Merges the weights of the model.\n    Call this after each update step.\n    \"\"\"\n    if self.clamp_weights:\n        layer_wise_weights = self.layer_wise_weights.clamp(0, 1)\n    else:\n        layer_wise_weights = self.layer_wise_weights\n    device = layer_wise_weights.device\n    task_vector = fuse_weights(layer_wise_weights, self.task_vectors)\n    self.merged_state_dict = {\n        k: self.pretrained_state_dict[k].to(device, non_blocking=True)\n        for k in self.pretrained_state_dict.keys()\n    }\n    for k in task_vector.keys():\n        self.merged_state_dict[k] = self.merged_state_dict[k] + task_vector[k]\n    if remove_keys is not None:\n        for k in remove_keys:\n            self.merged_state_dict.pop(k)\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.merge.wrapper.layer_wise_fusion.fuse_weights","title":"<code>fusionlib.merge.wrapper.layer_wise_fusion.fuse_weights(layer_wise_weight, state_dicts)</code>","text":"<p>Fuse the weights of multiple models using layer-wise fusion.</p> <p>Parameters:</p> <ul> <li> <code>layer_wise_weight</code>               (<code>Tensor</code>)           \u2013            <p>A tensor of shape (num_models, num_layers) representing the weight of each layer for each model.</p> </li> <li> <code>state_dicts</code>               (<code>List[StateDict]</code>)           \u2013            <p>A list of state dictionaries, one for each model.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>_StateDict</code>           \u2013            <p>A dictionary mapping each weight tensor key to the fused weight tensor.</p> </li> </ul> Source code in <code>fusionlib/merge/wrapper/layer_wise_fusion.py</code> <pre><code>def fuse_weights(\n    layer_wise_weight: Tensor, state_dicts: List[_StateDict]\n) -&gt; _StateDict:\n    \"\"\"\n    Fuse the weights of multiple models using layer-wise fusion.\n\n    Args:\n        layer_wise_weight (Tensor): A tensor of shape (num_models, num_layers) representing the weight of each layer for each model.\n        state_dicts (List[StateDict]): A list of state dictionaries, one for each model.\n\n    Returns:\n        A dictionary mapping each weight tensor key to the fused weight tensor.\n    \"\"\"\n    num_models = len(state_dicts)\n    num_layers = len(state_dicts[0])\n    assert layer_wise_weight.shape == (\n        num_models,\n        num_layers,\n    ), f\"layer_wise_weight.shape={layer_wise_weight.shape}, expected (num_models, num_layers): ({num_models}, {num_layers})\"\n    return {\n        k: _fuse_weights(\n            layer_wise_weight[:, i], [state_dict[k] for state_dict in state_dicts]\n        )\n        for i, k in enumerate(state_dicts[0].keys())\n    }\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.merge.wrapper.layer_wise_fusion.get_layer_wise_weights","title":"<code>fusionlib.merge.wrapper.layer_wise_fusion.get_layer_wise_weights(num_models, num_layers, init_values=None)</code>","text":"<p>Return a tensor of layer-wise weights for the given number of models and layers.</p> <p>Parameters:</p> <ul> <li> <code>num_models</code>               (<code>int</code>)           \u2013            <p>The number of models to fuse.</p> </li> <li> <code>num_layers</code>               (<code>int</code>)           \u2013            <p>The number of layers in each model.</p> </li> <li> <code>init_values</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>The initial value for each weight. Defaults to 1.0 / num_models.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>          \u2013            <p>A tensor of shape (num_models, num_layers) containing the layer-wise weights.</p> </li> </ul> Source code in <code>fusionlib/merge/wrapper/layer_wise_fusion.py</code> <pre><code>def get_layer_wise_weights(num_models: int, num_layers: int, init_values: float = None):\n    \"\"\"\n    Return a tensor of layer-wise weights for the given number of models and layers.\n\n    Args:\n        num_models (int): The number of models to fuse.\n        num_layers (int): The number of layers in each model.\n        init_values (float, optional): The initial value for each weight. Defaults to 1.0 / num_models.\n\n    Returns:\n        Tensor: A tensor of shape (num_models, num_layers) containing the layer-wise weights.\n    \"\"\"\n    assert num_models &gt;= 1, f\"num_models must be &gt;= 1, got {num_models}\"\n    assert num_layers &gt;= 1, f\"num_layers must be &gt;= 1, got {num_layers}\"\n    if init_values is None:\n        init_values = 1.0 / num_models\n    return torch.full((num_models, num_layers), init_values, dtype=torch.float32)\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.merge.wrapper.task_wise_fusion","title":"<code>fusionlib.merge.wrapper.task_wise_fusion</code>","text":"<pre><code># Get the task-wise weights\ntask_wise_weights = get_task_wise_weights(num_models)\n\n# Define the task vectors (in this case, we'll use the state_dict of the pretrained model)\ntask_vectors = ...\n\n# Initialize the TaskWiseMergedModel\nmerged_model = TaskWiseMergedModel(pretrained_model, task_wise_weights, task_vectors)\n\n# Now you can use the merged_model like a regular PyTorch model\noutputs = merged_model(inputs)\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.merge.wrapper.task_wise_fusion.fuse_weights","title":"<code>fusionlib.merge.wrapper.task_wise_fusion.fuse_weights(task_wise_weight, state_dicts)</code>","text":"<p>This function fuses the weights of the models and returns a state dictionary.</p> <p>Parameters:</p> <ul> <li> <code>task_wise_weight</code>               (<code>Tensor</code>)           \u2013            <p>The weights for each model. on cuda or cpu.</p> </li> <li> <code>state_dicts</code>               (<code>List[StateDict]</code>)           \u2013            <p>The list of state dictionaries. on cpu.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>StateDict</code> (              <code>_StateDict</code> )          \u2013            <p>The fused state dictionary.</p> </li> </ul> Source code in <code>fusionlib/merge/wrapper/task_wise_fusion.py</code> <pre><code>def fuse_weights(task_wise_weight: Tensor, state_dicts: List[_StateDict]) -&gt; _StateDict:\n    \"\"\"\n    This function fuses the weights of the models and returns a state dictionary.\n\n    Args:\n        task_wise_weight (Tensor): The weights for each model. on cuda or cpu.\n        state_dicts (List[StateDict]): The list of state dictionaries. on cpu.\n\n    Returns:\n        StateDict: The fused state dictionary.\n    \"\"\"\n    num_models = len(state_dicts)\n    assert (\n        task_wise_weight.dim() == 1\n    ), f\"task_wise_weight must be a 1D tensor, got {task_wise_weight.dim()}\"\n    assert num_models == task_wise_weight.size(\n        0\n    ), f\"num_models must be equal to the number of state_dicts, got {num_models} and {task_wise_weight.size(0)}\"\n    return {\n        k: _fuse_weights(task_wise_weight, [sd[k] for sd in state_dicts])\n        for k in state_dicts[0].keys()\n    }\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.merge.wrapper.task_wise_fusion.get_task_wise_weights","title":"<code>fusionlib.merge.wrapper.task_wise_fusion.get_task_wise_weights(num_models, init_values=None)</code>","text":"<p>This function generates a tensor of weights for each model.</p> <p>Parameters:</p> <ul> <li> <code>num_models</code>               (<code>int</code>)           \u2013            <p>The number of models.</p> </li> <li> <code>init_values</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>The initial value for each weight. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>          \u2013            <p>A tensor of weights for each model.</p> </li> </ul> Source code in <code>fusionlib/merge/wrapper/task_wise_fusion.py</code> <pre><code>def get_task_wise_weights(num_models: int, init_values: float = None):\n    \"\"\"\n    This function generates a tensor of weights for each model.\n\n    Args:\n        num_models (int): The number of models.\n        init_values (float, optional): The initial value for each weight. Defaults to None.\n\n    Returns:\n        Tensor: A tensor of weights for each model.\n    \"\"\"\n    assert num_models &gt;= 1, f\"num_models must be &gt;= 1, got {num_models}\"\n    if init_values is None:\n        init_values = 1.0 / num_models\n    return torch.full((num_models,), init_values, dtype=torch.float32)\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.utils","title":"<code>fusionlib.utils</code>","text":""},{"location":"api/fusionlib/#fusionlib.utils.args","title":"<code>fusionlib.utils.args</code>","text":""},{"location":"api/fusionlib/#fusionlib.utils.args.verify_str_arg","title":"<code>fusionlib.utils.args.verify_str_arg(value, arg=None, valid_values=None, custom_msg=None, to_lower=False)</code>","text":"<p>check is string argument <code>value</code> with name <code>arg</code> in <code>valid_values</code>, raise <code>ValueError</code> if failed.</p> <p>Examples:</p> <p>if you have a function <code>f</code> accept <code>batch_size</code> as argument, such as:</p> <pre><code>def f(batch_size='half'):\n    verify_str_arg(batch_size, 'batch_size', ['half', 'full'])\n    ...\n</code></pre> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>T</code>)           \u2013            <p><code>str</code> or <code>bytes</code></p> </li> <li> <code>arg</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>. Defaults to None.</p> </li> <li> <code>valid_values</code>               (<code>Iterable[T]</code>, default:                   <code>None</code> )           \u2013            <p>Defaults to <code>None</code>. if this is <code>None</code>, accept any string input.</p> </li> <li> <code>custom_msg</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Defaults to \"Unknown value '{value}' for argument {arg}. Valid values are {{{valid_values}}}.\".</p> </li> <li> <code>to_lower(bool)</code>           \u2013            <p>if <code>True</code>, accept uppercase value.</p> </li> </ul> <p>Raises:     ValueError:</p> <p>Returns:</p> <ul> <li> <code>T</code> (              <code>T</code> )          \u2013            <p>value</p> </li> </ul> Source code in <code>fusionlib/utils/args.py</code> <pre><code>def verify_str_arg(\n    value: T,\n    arg: Optional[str] = None,\n    valid_values: Iterable[T] = None,\n    custom_msg: Optional[str] = None,\n    to_lower: bool = False,\n) -&gt; T:\n    R\"\"\"\n    check is string argument ``value`` with name ``arg`` in `valid_values`, raise `ValueError` if failed.\n\n    Examples:\n\n    if you have a function ``f`` accept `batch_size` as argument, such as:\n\n    ```python\n    def f(batch_size='half'):\n        verify_str_arg(batch_size, 'batch_size', ['half', 'full'])\n        ...\n    ```\n\n    Args:\n        value (T): `str` or `bytes`\n        arg (Optional[str], optional): . Defaults to None.\n        valid_values (Iterable[T], optional): Defaults to `None`. if this is `None`, accept any string input.\n        custom_msg (Optional[str], optional): Defaults to \"Unknown value '{value}' for argument {arg}. Valid values are {{{valid_values}}}.\".\n        to_lower(bool): if `True`, accept uppercase value.\n    Raises:\n        ValueError:\n\n    Returns:\n        T: value\n    \"\"\"\n    if not isinstance(value, (str, bytes)):\n        if arg is None:\n            msg = \"Expected type str, but got type {type}.\"\n        else:\n            msg = \"Expected type str for argument {arg}, but got type {type}.\"\n        msg = msg.format(type=type(value), arg=arg)\n        raise ValueError(msg)\n    else:\n        if valid_values is None:\n            return value\n\n        if to_lower:\n            value = value.lower()\n\n        if value not in valid_values:\n            if custom_msg is not None:\n                msg = custom_msg\n            else:\n                msg = \"Unknown value '{value}' for argument {arg}. Valid values are {{{valid_values}}}.\"\n                msg = msg.format(\n                    value=value, arg=arg, valid_values=_iterable_to_str(valid_values)\n                )\n            raise ValueError(msg)\n\n    return value\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.utils.devices","title":"<code>fusionlib.utils.devices</code>","text":""},{"location":"api/fusionlib/#fusionlib.utils.devices.num_devices","title":"<code>fusionlib.utils.devices.num_devices(devices)</code>","text":"<p>Return the number of devices.</p> <p>Parameters:</p> <ul> <li> <code>devices</code>               (<code>Union[int, List[int], str]</code>)           \u2013            <p><code>devices</code> can be a single int to specify the number of devices, or a list of device ids, e.g. [0, 1, 2, 3]\uff0c or a str of device ids, e.g. \"0,1,2,3\" and \"[0, 1, 2]\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>The number of devices.</p> </li> </ul> Source code in <code>fusionlib/utils/devices.py</code> <pre><code>def num_devices(devices: Union[int, List[int], str]) -&gt; int:\n    \"\"\"\n    Return the number of devices.\n\n    Args:\n        devices: `devices` can be a single int to specify the number of devices, or a list of device ids, e.g. [0, 1, 2, 3]\uff0c or a str of device ids, e.g. \"0,1,2,3\" and \"[0, 1, 2]\".\n\n    Returns:\n        The number of devices.\n    \"\"\"\n    if isinstance(devices, int):\n        return devices\n    elif isinstance(devices, str):\n        return len(devices.split(\",\"))\n    elif isinstance(devices, list):\n        return len(devices)\n    else:\n        raise TypeError(\n            f\"devices must be a single int or a list of ints, but got {type(devices)}\"\n        )\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.utils.path","title":"<code>fusionlib.utils.path</code>","text":""},{"location":"api/fusionlib/#fusionlib.utils.path.listdir_fullpath","title":"<code>fusionlib.utils.path.listdir_fullpath(dir)</code>","text":"<p>list directory <code>dir</code>, return fullpaths</p> <p>Parameters:</p> <ul> <li> <code>dir</code>               (<code>Union[str, Path]</code>)           \u2013            <p>directory name or Path object</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[List[str], List[Path]]</code>           \u2013            <p>Union[List[str], List[Path]]: a list of fullpaths</p> </li> </ul> Source code in <code>fusionlib/utils/path.py</code> <pre><code>def listdir_fullpath(dir: Union[str, Path]) -&gt; Union[List[str], List[Path]]:\n    \"\"\"list directory `dir`, return fullpaths\n\n    Args:\n        dir (Union[str, Path]): directory name or Path object\n\n    Returns:\n        Union[List[str], List[Path]]: a list of fullpaths\n    \"\"\"\n    assert os.path.isdir(dir), \"Argument 'dir' must be a Directory\"\n    if isinstance(dir, str):\n        names = os.listdir(dir)\n        return [os.path.join(dir, name) for name in names]\n    elif isinstance(dir, Path):\n        names = dir.iterdir()\n        return [dir / i for i in names]\n    else:\n        raise TypeError(\"Argument 'dir' must be a string or a Path object\")\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.utils.rich_logging","title":"<code>fusionlib.utils.rich_logging</code>","text":"<p>This module provides utilities for logging with rich output formatting.</p> <p>It defines a function <code>setup_colorlogging</code> that sets up a logging configuration with a <code>RichHandler</code> that outputs log messages with rich formatting to the console. It also defines two functions <code>pprint_code</code> and <code>pprint_yaml</code> that pretty print code snippets in different formats using the <code>Syntax</code> class from the <code>rich.syntax</code> module.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from utils.logging.rich import setup_colorlogging, pprint_yaml\n\n&gt;&gt;&gt; setup_colorlogging()\n&gt;&gt;&gt; pprint_yaml('foo: bar')\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.utils.rich_logging.pprint_json","title":"<code>fusionlib.utils.rich_logging.pprint_json = partial(pprint_code, format='json')</code>  <code>module-attribute</code>","text":"<p>pretty print json code</p>"},{"location":"api/fusionlib/#fusionlib.utils.rich_logging.pprint_yaml","title":"<code>fusionlib.utils.rich_logging.pprint_yaml = partial(pprint_code, format='yaml')</code>  <code>module-attribute</code>","text":"<p>pretty print yaml code</p>"},{"location":"api/fusionlib/#fusionlib.utils.rich_logging.pprint_code","title":"<code>fusionlib.utils.rich_logging.pprint_code(code, format)</code>","text":"<p>pretty print code</p> <p>Parameters:</p> <ul> <li> <code>code</code>               (<code>str</code>)           \u2013            <p>code str</p> </li> <li> <code>format</code>               (<code>str</code>)           \u2013            <p>code format, for example 'yaml'</p> </li> </ul> Source code in <code>fusionlib/utils/rich_logging.py</code> <pre><code>def pprint_code(code: str, format: str):\n    \"\"\"pretty print code\n\n    Args:\n        code (str): code str\n        format (str): code format, for example 'yaml'\n    \"\"\"\n    s = Syntax(code, format)\n    _console.print(s)\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.utils.timer","title":"<code>fusionlib.utils.timer</code>","text":""},{"location":"api/fusionlib/#fusionlib.utils.timer.timer","title":"<code>fusionlib.utils.timer.timer</code>","text":"<p>Usage:</p> <pre><code>```python\n# Usage as a decorator\n@timer\ndef my_func():\n    time.sleep(1)\n\nmy_func()\n\n# Usage as a context manager\nwith timer():\n    time.sleep(1)\n```\n</code></pre> Source code in <code>fusionlib/utils/timer.py</code> <pre><code>class timer:\n    \"\"\"\n    Usage:\n\n        ```python\n        # Usage as a decorator\n        @timer\n        def my_func():\n            time.sleep(1)\n\n        my_func()\n\n        # Usage as a context manager\n        with timer():\n            time.sleep(1)\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        func_or_msg: Optional[Union[Callable, str]] = None,\n        msg: Optional[str] = None,\n        logger: Optional[logging.Logger] = None,\n        loglevel=logging.INFO,\n        human_readable: bool = True,\n    ) -&gt; None:\n        if isinstance(func_or_msg, str):\n            self.func = None\n            self.msg = func_or_msg\n        else:\n            self.func = func_or_msg\n            self.msg = msg\n        self.loglevel = loglevel\n        self.human_readable = human_readable\n\n        if logger is None:\n            self._logger_fn = print\n        else:\n            self._logger_fn = functools.partial(\n                logger.log, level=loglevel, stacklevel=3\n            )\n\n    @property\n    def elapsed_time(self):\n        return time.time() - self.start_time\n\n    def reset(self):\n        self.start_time = time.time()\n\n    def __enter__(self) -&gt; None:\n        self.start_time = time.time()\n        if self.msg is None:\n            if self.func is not None:\n                self.msg = self.func.__name__\n            else:\n                self.msg = \"\"\n        self._logger_fn(f\"[START] {self.msg}\")\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"\n        Calculates the elapsed time and logs it, along with an optional message indicating the end of the code block execution.\n        \"\"\"\n        elapsed_time = self.elapsed_time\n        self._logger_fn(f\"[END] {self.msg} {elapsed_time:.2f}s\")\n\n        # If an exception occurred, log it\n        if exc_type is not None:\n            logger.error(f\"An error occurred: {exc_val}\")\n            # Return True to suppress the exception, False otherwise\n            return False\n\n    def __call__(self, *args, **kwargs):\n        self.__enter__()\n        try:\n            assert self.func is not None, \"func must be set if using as a decorator\"\n            return self.func(*args, **kwargs)\n        except Exception as e:\n            # Pass exception info to __exit__\n            exc_type, exc_val, exc_tb = sys.exc_info()\n            self.__exit__(exc_type, exc_val, exc_tb)\n            raise  # re-raise the exception\n        finally:\n            # No exception, so pass None as all three arguments\n            self.__exit__(None, None, None)\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.utils.timer.timer.__exit__","title":"<code>fusionlib.utils.timer.timer.__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Calculates the elapsed time and logs it, along with an optional message indicating the end of the code block execution.</p> Source code in <code>fusionlib/utils/timer.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"\n    Calculates the elapsed time and logs it, along with an optional message indicating the end of the code block execution.\n    \"\"\"\n    elapsed_time = self.elapsed_time\n    self._logger_fn(f\"[END] {self.msg} {elapsed_time:.2f}s\")\n\n    # If an exception occurred, log it\n    if exc_type is not None:\n        logger.error(f\"An error occurred: {exc_val}\")\n        # Return True to suppress the exception, False otherwise\n        return False\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.utils.torch","title":"<code>fusionlib.utils.torch</code>","text":""},{"location":"api/fusionlib/#fusionlib.utils.torch.parameters","title":"<code>fusionlib.utils.torch.parameters</code>","text":""},{"location":"api/fusionlib/#fusionlib.utils.torch.parameters.check_parameters_all_equal","title":"<code>fusionlib.utils.torch.parameters.check_parameters_all_equal(list_of_param_names)</code>","text":"<p>Checks if all models have the same parameters.</p> <p>This function takes a list of parameter names or state dictionaries from different models. It checks if all models have the same parameters by comparing the parameter names. If any model has different parameters, it raises a ValueError with the differing parameters.</p> <p>Parameters:</p> <ul> <li> <code>list_of_param_names</code>               (<code>List[Union[StateDict, List[str]]]</code>)           \u2013            <p>A list of parameter names or state dictionaries.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If any model has different parameters.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>None</p> </li> </ul> Source code in <code>fusionlib/utils/torch/parameters.py</code> <pre><code>def check_parameters_all_equal(\n    list_of_param_names: List[Union[_StateDict, nn.Module, List[str]]]\n) -&gt; None:\n    \"\"\"\n    Checks if all models have the same parameters.\n\n    This function takes a list of parameter names or state dictionaries from different models.\n    It checks if all models have the same parameters by comparing the parameter names.\n    If any model has different parameters, it raises a ValueError with the differing parameters.\n\n    Args:\n        list_of_param_names (List[Union[StateDict, List[str]]]): A list of parameter names or state dictionaries.\n\n    Raises:\n        ValueError: If any model has different parameters.\n\n    Returns:\n        None\n    \"\"\"\n    if isinstance(list_of_param_names[0], Mapping):\n        list_of_param_names = [list(i.keys()) for i in list_of_param_names]\n    elif isinstance(list_of_param_names[0], nn.Module):\n        list_of_param_names = [list(i.state_dict().keys()) for i in list_of_param_names]\n    else:\n        parameter_names = set(list_of_param_names[0])\n\n        if len(list_of_param_names) &gt;= 2:\n            # raise ValueError(\"Number of models is less than 2.\")\n            for names in list_of_param_names[1:]:\n                current_parameterNames = set(names)\n                if current_parameterNames != parameter_names:\n                    raise ValueError(\n                        \"Differing parameter names in models. \"\n                        f\"The different parameters are {parameter_names.symmetric_difference(current_parameterNames)}\"\n                    )\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.utils.torch.parameters.count_parameters","title":"<code>fusionlib.utils.torch.parameters.count_parameters(module)</code>","text":"<p>Counts the number of trainable and total parameters in a PyTorch model.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The PyTorch model for which to count parameters.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple</code>          \u2013            <p>A tuple containing the number of trainable parameters and the total number of parameters.</p> </li> </ul> <p>Examples:</p> <pre><code>```python\n# Count the parameters\ntrainable_params, all_params = count_parameters(model)\n```\n</code></pre> Source code in <code>fusionlib/utils/torch/parameters.py</code> <pre><code>@torch.no_grad()\ndef count_parameters(module: nn.Module):\n    \"\"\"\n    Counts the number of trainable and total parameters in a PyTorch model.\n\n    Args:\n        model (nn.Module): The PyTorch model for which to count parameters.\n\n    Returns:\n        tuple: A tuple containing the number of trainable parameters and the total number of parameters.\n\n    Examples:\n\n        ```python\n        # Count the parameters\n        trainable_params, all_params = count_parameters(model)\n        ```\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for name, param in module.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    return trainable_params, all_param\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.utils.torch.parameters.print_parameters","title":"<code>fusionlib.utils.torch.parameters.print_parameters(module, human_readable=True)</code>","text":"<p>Prints the number of trainable and total parameters in a PyTorch model.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>Module</code>)           \u2013            <p>The PyTorch model for which to print parameters.</p> </li> <li> <code>human_readable</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, the parameter counts are converted to a human-readable format (e.g., '1.5M' instead of '1500000'). Defaults to True.</p> </li> </ul> Prints <p>The number of trainable parameters, the total number of parameters, and the percentage of trainable parameters in the model.</p> Source code in <code>fusionlib/utils/torch/parameters.py</code> <pre><code>def print_parameters(\n    module: nn.Module,\n    human_readable: bool = True,\n):\n    \"\"\"\n    Prints the number of trainable and total parameters in a PyTorch model.\n\n    Args:\n        module (nn.Module): The PyTorch model for which to print parameters.\n        human_readable (bool, optional): If True, the parameter counts are converted to a human-readable format (e.g., '1.5M' instead of '1500000'). Defaults to True.\n\n    Prints:\n        The number of trainable parameters, the total number of parameters, and the percentage of trainable parameters in the model.\n    \"\"\"\n    trainable_params, all_param = count_parameters(module)\n    if human_readable:\n        trainable_params = human_readable(trainable_params)\n        all_param = human_readable(all_param)\n\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.4f}\"\n    )\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.utils.torch.state_dict","title":"<code>fusionlib.utils.torch.state_dict</code>","text":""},{"location":"api/fusionlib/#fusionlib.utils.torch.state_dict.StateDict","title":"<code>fusionlib.utils.torch.state_dict.StateDict</code>","text":"<p>A wrapper class for PyTorch state dictionaries.</p> <p>This class allows arithmetic operations to be performed directly on state dictionaries. It can be initialized with a PyTorch module, a state dictionary, or another StateDict instance.</p> <p>Parameters:</p> <ul> <li> <code>state_dict</code>               (<code>Union[_StateDict, StateDict, Module]</code>)           \u2013            <p>The initial state dictionary. This can be a PyTorch module, a state dictionary, or another StateDict instance.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the state_dict argument is not a PyTorch module, a state dictionary, or a StateDict instance.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch.nn as nn\n&gt;&gt;&gt; model = nn.Linear(10, 10)\n&gt;&gt;&gt; state_dict = StateDict(model)\n&gt;&gt;&gt; print(state_dict['weight'])  # Access state dictionary items directly\n&gt;&gt;&gt; state_dict2 = state_dict + 0.1  # Add a scalar to all parameters\n&gt;&gt;&gt; state_dict3 = state_dict * 0.9  # Multiply all parameters by a scalar\n</code></pre> Source code in <code>fusionlib/utils/torch/state_dict.py</code> <pre><code>class StateDict:\n    \"\"\"\n    A wrapper class for PyTorch state dictionaries.\n\n    This class allows arithmetic operations to be performed directly on state dictionaries. It can be initialized with a PyTorch module, a state dictionary, or another StateDict instance.\n\n    Args:\n        state_dict (Union[_StateDict, \"StateDict\", nn.Module]): The initial state dictionary. This can be a PyTorch module, a state dictionary, or another StateDict instance.\n\n    Raises:\n        TypeError: If the state_dict argument is not a PyTorch module, a state dictionary, or a StateDict instance.\n\n    Examples:\n        &gt;&gt;&gt; import torch.nn as nn\n        &gt;&gt;&gt; model = nn.Linear(10, 10)\n        &gt;&gt;&gt; state_dict = StateDict(model)\n        &gt;&gt;&gt; print(state_dict['weight'])  # Access state dictionary items directly\n        &gt;&gt;&gt; state_dict2 = state_dict + 0.1  # Add a scalar to all parameters\n        &gt;&gt;&gt; state_dict3 = state_dict * 0.9  # Multiply all parameters by a scalar\n    \"\"\"\n\n    def __init__(self, state_dict: Union[_StateDict, \"StateDict\", nn.Module]):\n        if isinstance(state_dict, nn.Module):\n            state_dict = state_dict.state_dict()\n        elif isinstance(state_dict, Mapping):\n            self.state_dict = state_dict\n        elif isinstance(state_dict, StateDict):\n            self.state_dict = state_dict.state_dict\n        else:\n            raise TypeError(\n                f\"unsupported type for state_dict: '{type(state_dict).__name__}'\"\n            )\n\n    def __add__(self, other: Union[\"StateDict\", _StateDict, Number]) -&gt; \"StateDict\":\n        if isinstance(other, (StateDict, Mapping)):\n            return StateDict(state_dict_add(self, other))\n        elif isinstance(other, Number):\n            return StateDict(state_dict_add_scalar(self, other))\n        else:\n            raise TypeError(\n                f\"unsupported operand type(s) for +: 'StateDict' and '{type(other).__name__}'\"\n            )\n\n    def __iadd__(self, other: Number) -&gt; \"StateDict\":\n        if isinstance(other, Number):\n            for key in self.state_dict:\n                self.state_dict[key] += other\n        else:\n            raise TypeError(\n                f\"unsupported operand type(s) for +: 'StateDict' and '{type(other).__name__}'\"\n            )\n\n    def __sub__(self, other: Union[\"StateDict\", float]) -&gt; \"StateDict\":\n        if isinstance(other, (StateDict, Mapping)):\n            return StateDict(state_dict_sub(self, other))\n        elif isinstance(other, Number):\n            return StateDict(state_dict_add_scalar(self, -other))\n        else:\n            raise TypeError(\n                f\"unsupported operand type(s) for -: 'StateDict' and '{type(other).__name__}'\"\n            )\n\n    def __isub__(self, other: Number) -&gt; \"StateDict\":\n        if isinstance(other, Number):\n            for key in self.state_dict:\n                self.state_dict[key] -= other\n        else:\n            raise TypeError(\n                f\"unsupported operand type(s) for -: 'StateDict' and '{type(other).__name__}'\"\n            )\n\n    def __mul__(self, other: Number) -&gt; \"StateDict\":\n        if isinstance(other, Number):\n            return StateDict(state_dict_mul(self, other))\n        else:\n            raise TypeError(\n                f\"unsupported operand type(s) for *: 'StateDict' and '{type(other).__name__}'\"\n            )\n\n    def __getattr__(self, name: str) -&gt; Any:\n        return getattr(self.state_dict, name)\n\n    def to_vector(self) -&gt; torch.Tensor:\n        \"\"\"\n        Convert the state dictionary to a vector.\n\n        Returns:\n            torch.Tensor: The state dictionary as a vector.\n        \"\"\"\n        return torch.nn.utils.parameters_to_vector(self.state_dict.values())\n\n    def from_vector(self, vector: torch.Tensor):\n        \"\"\"\n        Convert a vector to a state dictionary.\n\n        Args:\n            vector (torch.Tensor): The vector to convert to a state dictionary.\n        \"\"\"\n        torch.nn.utils.vector_to_parameters(vector, self.state_dict.values())\n        return self\n</code></pre> <code></code> <code>fusionlib.utils.torch.state_dict.StateDict.from_vector(vector)</code> \u00b6 <p>Convert a vector to a state dictionary.</p> <p>Parameters:</p> <ul> <li> <code>vector</code>               (<code>Tensor</code>)           \u2013            <p>The vector to convert to a state dictionary.</p> </li> </ul> Source code in <code>fusionlib/utils/torch/state_dict.py</code> <pre><code>def from_vector(self, vector: torch.Tensor):\n    \"\"\"\n    Convert a vector to a state dictionary.\n\n    Args:\n        vector (torch.Tensor): The vector to convert to a state dictionary.\n    \"\"\"\n    torch.nn.utils.vector_to_parameters(vector, self.state_dict.values())\n    return self\n</code></pre> <code></code> <code>fusionlib.utils.torch.state_dict.StateDict.to_vector()</code> \u00b6 <p>Convert the state dictionary to a vector.</p> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>torch.Tensor: The state dictionary as a vector.</p> </li> </ul> Source code in <code>fusionlib/utils/torch/state_dict.py</code> <pre><code>def to_vector(self) -&gt; torch.Tensor:\n    \"\"\"\n    Convert the state dictionary to a vector.\n\n    Returns:\n        torch.Tensor: The state dictionary as a vector.\n    \"\"\"\n    return torch.nn.utils.parameters_to_vector(self.state_dict.values())\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.utils.torch.state_dict_arithmetic","title":"<code>fusionlib.utils.torch.state_dict_arithmetic</code>","text":""},{"location":"api/fusionlib/#fusionlib.utils.torch.state_dict_arithmetic.num_params_of_state_dict","title":"<code>fusionlib.utils.torch.state_dict_arithmetic.num_params_of_state_dict(state_dict)</code>","text":"<p>Returns the number of parameters in a state dict.</p> <p>Parameters:</p> <ul> <li> <code>state_dict</code>               (<code>Dict[str, Tensor]</code>)           \u2013            <p>The state dict to count the number of parameters in.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>          \u2013            <p>The number of parameters in the state dict.</p> </li> </ul> Source code in <code>fusionlib/utils/torch/state_dict_arithmetic.py</code> <pre><code>def num_params_of_state_dict(state_dict: _StateDict):\n    \"\"\"\n    Returns the number of parameters in a state dict.\n\n    Args:\n        state_dict (Dict[str, Tensor]): The state dict to count the number of parameters in.\n\n    Returns:\n        int: The number of parameters in the state dict.\n    \"\"\"\n    return sum([state_dict[key].numel() for key in state_dict])\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.utils.torch.state_dict_arithmetic.state_dict_add","title":"<code>fusionlib.utils.torch.state_dict_arithmetic.state_dict_add(a, b, strict=True, device=None)</code>","text":"<p>Returns the sum of two state dicts.</p> <p>Parameters:</p> <ul> <li> <code>a</code>               (<code>Dict</code>)           \u2013            <p>The first state dict.</p> </li> <li> <code>b</code>               (<code>Dict</code>)           \u2013            <p>The second state dict.</p> </li> <li> <code>strict</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to check if the keys of the two state dicts are the same.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code>          \u2013            <p>The sum of the two state dicts.</p> </li> </ul> Source code in <code>fusionlib/utils/torch/state_dict_arithmetic.py</code> <pre><code>def state_dict_add(a: _StateDict, b: _StateDict, strict: bool = True, device=None):\n    \"\"\"\n    Returns the sum of two state dicts.\n\n    Args:\n        a (Dict): The first state dict.\n        b (Dict): The second state dict.\n        strict (bool): Whether to check if the keys of the two state dicts are the same.\n\n    Returns:\n        Dict: The sum of the two state dicts.\n    \"\"\"\n    ans = OrderedDict()\n    if strict:\n        check_parameters_all_equal([a, b])\n        for key in a:\n            ans[key] = a[key] + b[key]\n    else:\n        for key in a:\n            if key in b:\n                ans[key] = a[key] + b[key]\n    if device is not None:\n        ans = to_device(ans, device)\n    return ans\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.utils.torch.state_dict_arithmetic.state_dict_avg","title":"<code>fusionlib.utils.torch.state_dict_arithmetic.state_dict_avg(state_dicts)</code>","text":"<p>Returns the average of a list of state dicts.</p> <p>Parameters:</p> <ul> <li> <code>state_dicts</code>               (<code>List[Dict[str, Tensor]]</code>)           \u2013            <p>The list of state dicts to average.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code>          \u2013            <p>The average of the state dicts.</p> </li> </ul> Source code in <code>fusionlib/utils/torch/state_dict_arithmetic.py</code> <pre><code>def state_dict_avg(state_dicts: List[_StateDict]):\n    \"\"\"\n    Returns the average of a list of state dicts.\n\n    Args:\n        state_dicts (List[Dict[str, Tensor]]): The list of state dicts to average.\n\n    Returns:\n        Dict: The average of the state dicts.\n    \"\"\"\n    assert len(state_dicts) &gt; 0, \"The number of state_dicts must be greater than 0\"\n    assert all(\n        [len(state_dicts[0]) == len(state_dict) for state_dict in state_dicts]\n    ), \"All state_dicts must have the same number of keys\"\n\n    num_state_dicts = len(state_dicts)\n    avg_state_dict = OrderedDict()\n    for key in state_dicts[0]:\n        avg_state_dict[key] = torch.zeros_like(state_dicts[0][key])\n        for state_dict in state_dicts:\n            avg_state_dict[key] += state_dict[key]\n        avg_state_dict[key] /= num_state_dicts\n    return avg_state_dict\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.utils.torch.state_dict_arithmetic.state_dict_flatten","title":"<code>fusionlib.utils.torch.state_dict_arithmetic.state_dict_flatten(state_dict)</code>","text":"<p>Flattens a state dict.</p> <p>Parameters:</p> <ul> <li> <code>state_dict</code>               (<code>Dict[str, Tensor]</code>)           \u2013            <p>The state dict to be flattened.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>          \u2013            <p>The flattened state dict.</p> </li> </ul> Source code in <code>fusionlib/utils/torch/state_dict_arithmetic.py</code> <pre><code>def state_dict_flatten(state_dict: Dict[str, Tensor]):\n    \"\"\"\n    Flattens a state dict.\n\n    Args:\n        state_dict (Dict[str, Tensor]): The state dict to be flattened.\n\n    Returns:\n        Tensor: The flattened state dict.\n    \"\"\"\n    flattened_state_dict = []\n    for key in state_dict:\n        flattened_state_dict.append(state_dict[key].flatten())\n    return torch.cat(flattened_state_dict)\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.utils.torch.state_dict_arithmetic.state_dict_interpolation","title":"<code>fusionlib.utils.torch.state_dict_arithmetic.state_dict_interpolation(state_dicts, scalars)</code>","text":"<p>Interpolates between a list of state dicts using a list of scalars.</p> <p>Parameters:</p> <ul> <li> <code>state_dicts</code>               (<code>List[Dict[str, Tensor]]</code>)           \u2013            <p>The list of state dicts to interpolate between.</p> </li> <li> <code>scalars</code>               (<code>List[float]</code>)           \u2013            <p>The list of scalars to use for interpolation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code>          \u2013            <p>The interpolated state dict.</p> </li> </ul> Source code in <code>fusionlib/utils/torch/state_dict_arithmetic.py</code> <pre><code>def state_dict_interpolation(\n    state_dicts: List[Dict[str, Tensor]], scalars: List[float]\n):\n    \"\"\"\n    Interpolates between a list of state dicts using a list of scalars.\n\n    Args:\n        state_dicts (List[Dict[str, Tensor]]): The list of state dicts to interpolate between.\n        scalars (List[float]): The list of scalars to use for interpolation.\n\n    Returns:\n        Dict: The interpolated state dict.\n    \"\"\"\n    assert len(state_dicts) == len(\n        scalars\n    ), \"The number of state_dicts and scalars must be the same\"\n    assert len(state_dicts) &gt; 0, \"The number of state_dicts must be greater than 0\"\n    assert all(\n        [len(state_dicts[0]) == len(state_dict) for state_dict in state_dicts]\n    ), \"All state_dicts must have the same number of keys\"\n\n    interpolated_state_dict = {}\n    for key in state_dicts[0]:\n        interpolated_state_dict[key] = torch.zeros_like(state_dicts[0][key])\n        for state_dict, scalar in zip(state_dicts, scalars):\n            interpolated_state_dict[key] += scalar * state_dict[key]\n    return interpolated_state_dict\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.utils.torch.state_dict_arithmetic.state_dict_mul","title":"<code>fusionlib.utils.torch.state_dict_arithmetic.state_dict_mul(state_dict, scalar)</code>","text":"<p>Returns the product of a state dict and a scalar.</p> <p>Parameters:</p> <ul> <li> <code>state_dict</code>               (<code>Dict</code>)           \u2013            <p>The state dict to be multiplied.</p> </li> <li> <code>scalar</code>               (<code>float</code>)           \u2013            <p>The scalar to multiply the state dict with.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code>          \u2013            <p>The product of the state dict and the scalar.</p> </li> </ul> Source code in <code>fusionlib/utils/torch/state_dict_arithmetic.py</code> <pre><code>def state_dict_mul(state_dict: _StateDict, scalar: float):\n    \"\"\"\n    Returns the product of a state dict and a scalar.\n\n    Args:\n        state_dict (Dict): The state dict to be multiplied.\n        scalar (float): The scalar to multiply the state dict with.\n\n    Returns:\n        Dict: The product of the state dict and the scalar.\n    \"\"\"\n    diff = OrderedDict()\n    for k in state_dict:\n        diff[k] = scalar * state_dict[k]\n    return diff\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.utils.torch.state_dict_arithmetic.state_dict_power","title":"<code>fusionlib.utils.torch.state_dict_arithmetic.state_dict_power(state_dict, p)</code>","text":"<p>Returns the power of a state dict.</p> <p>Parameters:</p> <ul> <li> <code>state_dict</code>               (<code>Dict[str, Tensor]</code>)           \u2013            <p>The state dict to be powered.</p> </li> <li> <code>p</code>               (<code>float</code>)           \u2013            <p>The power to raise the state dict to.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Dict[str, Tensor]: The powered state dict.</p> </li> </ul> Source code in <code>fusionlib/utils/torch/state_dict_arithmetic.py</code> <pre><code>def state_dict_power(state_dict: Dict[str, Tensor], p: float):\n    \"\"\"\n    Returns the power of a state dict.\n\n    Args:\n        state_dict (Dict[str, Tensor]): The state dict to be powered.\n        p (float): The power to raise the state dict to.\n\n    Returns:\n        Dict[str, Tensor]: The powered state dict.\n    \"\"\"\n    powered_state_dict = {}\n    for key in state_dict:\n        powered_state_dict[key] = state_dict[key] ** p\n    return powered_state_dict\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.utils.torch.state_dict_arithmetic.state_dict_sub","title":"<code>fusionlib.utils.torch.state_dict_arithmetic.state_dict_sub(a, b, strict=True, device=None)</code>","text":"<p>Returns the difference between two state dicts <code>a-b</code>.</p> <p>Parameters:</p> <ul> <li> <code>a</code>               (<code>_StateDict</code>)           \u2013            <p>The first state dict.</p> </li> <li> <code>b</code>               (<code>_StateDict</code>)           \u2013            <p>The second state dict.</p> </li> <li> <code>strict</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to check if the keys of the two state dicts are the same.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>_StateDict</code>          \u2013            <p>The difference between the two state dicts.</p> </li> </ul> Source code in <code>fusionlib/utils/torch/state_dict_arithmetic.py</code> <pre><code>def state_dict_sub(a: _StateDict, b: _StateDict, strict: bool = True, device=None):\n    \"\"\"\n    Returns the difference between two state dicts `a-b`.\n\n    Args:\n        a (_StateDict): The first state dict.\n        b (_StateDict): The second state dict.\n        strict (bool): Whether to check if the keys of the two state dicts are the same.\n\n    Returns:\n        _StateDict: The difference between the two state dicts.\n    \"\"\"\n    if strict:\n        assert set(a.keys()) == set(b.keys())\n\n    diff = OrderedDict()\n    for k in a:\n        if k in b:\n            diff[k] = a[k] - b[k]\n            if device is not None:\n                diff[k] = diff[k].to(device, non_blocking=True)\n    return diff\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.utils.torch.state_dict_arithmetic.state_dict_sum","title":"<code>fusionlib.utils.torch.state_dict_arithmetic.state_dict_sum(state_dicts)</code>","text":"<p>Returns the sum of a list of state dicts.</p> <p>Parameters:</p> <ul> <li> <code>state_dicts</code>               (<code>List[Dict[str, Tensor]]</code>)           \u2013            <p>The list of state dicts to sum.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code>          \u2013            <p>The sum of the state dicts.</p> </li> </ul> Source code in <code>fusionlib/utils/torch/state_dict_arithmetic.py</code> <pre><code>def state_dict_sum(state_dicts: List[_StateDict]):\n    \"\"\"\n    Returns the sum of a list of state dicts.\n\n    Args:\n        state_dicts (List[Dict[str, Tensor]]): The list of state dicts to sum.\n\n    Returns:\n        Dict: The sum of the state dicts.\n    \"\"\"\n    assert len(state_dicts) &gt; 0, \"The number of state_dicts must be greater than 0\"\n    assert all(\n        [len(state_dicts[0]) == len(state_dict) for state_dict in state_dicts]\n    ), \"All state_dicts must have the same number of keys\"\n\n    sum_state_dict = OrderedDict()\n    for key in state_dicts[0]:\n        sum_state_dict[key] = torch.zeros_like(state_dicts[0][key])\n        for state_dict in state_dicts:\n            sum_state_dict[key] += state_dict[key]\n    return sum_state_dict\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.utils.torch.state_dict_arithmetic.state_dict_weighted_sum","title":"<code>fusionlib.utils.torch.state_dict_arithmetic.state_dict_weighted_sum(state_dicts, weights, device=None)</code>","text":"<p>Returns the weighted sum of a list of state dicts.</p> <p>Parameters:</p> <ul> <li> <code>state_dicts</code>               (<code>List[Dict[str, Tensor]]</code>)           \u2013            <p>The list of state dicts to interpolate between.</p> </li> <li> <code>weights</code>               (<code>List[float]</code>)           \u2013            <p>The list of weights to use for the weighted sum.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code>          \u2013            <p>The weighted sum of the state dicts.</p> </li> </ul> Source code in <code>fusionlib/utils/torch/state_dict_arithmetic.py</code> <pre><code>def state_dict_weighted_sum(\n    state_dicts: List[Dict[str, Tensor]], weights: List[float], device=None\n):\n    \"\"\"\n    Returns the weighted sum of a list of state dicts.\n\n    Args:\n        state_dicts (List[Dict[str, Tensor]]): The list of state dicts to interpolate between.\n        weights (List[float]): The list of weights to use for the weighted sum.\n\n    Returns:\n        Dict: The weighted sum of the state dicts.\n    \"\"\"\n    assert len(state_dicts) == len(\n        weights\n    ), \"The number of state_dicts and weights must be the same\"\n    assert len(state_dicts) &gt; 0, \"The number of state_dicts must be greater than 0\"\n    assert all(\n        [len(state_dicts[0]) == len(state_dict) for state_dict in state_dicts]\n    ), \"All state_dicts must have the same number of keys\"\n\n    weighted_sum_state_dict = {}\n    for key in state_dicts[0]:\n        weighted_sum_state_dict[key] = torch.zeros_like(state_dicts[0][key])\n        for state_dict, weight in zip(state_dicts, weights):\n            if device is None:\n                weighted_sum_state_dict[key] += weight * state_dict[key]\n            else:\n                # NOTE: if weight is a tensor, state_dict and weight must be on the same device\n                weighted_sum_state_dict[key] += (weight * state_dict[key]).to(\n                    device, non_blocking=True\n                )\n    return weighted_sum_state_dict\n    return weighted_sum_state_dict\n</code></pre>"},{"location":"api/fusionlib/#fusionlib.utils.torch.state_dict_arithmetic.state_dicts_check_keys","title":"<code>fusionlib.utils.torch.state_dict_arithmetic.state_dicts_check_keys(state_dicts)</code>","text":"<p>Checks that the state dictionaries have the same keys.</p> <p>Parameters:</p> <ul> <li> <code>state_dicts</code>               (<code>List[Dict[str, Tensor]]</code>)           \u2013            <p>A list of dictionaries containing the state of PyTorch models.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the state dictionaries have different keys.</p> </li> </ul> Source code in <code>fusionlib/utils/torch/state_dict_arithmetic.py</code> <pre><code>def state_dicts_check_keys(state_dicts: List[_StateDict]):\n    \"\"\"\n    Checks that the state dictionaries have the same keys.\n\n    Args:\n        state_dicts (List[Dict[str, Tensor]]): A list of dictionaries containing the state of PyTorch models.\n\n    Raises:\n        ValueError: If the state dictionaries have different keys.\n    \"\"\"\n    # Get the keys of the first state dictionary in the list\n    keys = set(state_dicts[0].keys())\n    # Check that all the state dictionaries have the same keys\n    for state_dict in state_dicts:\n        assert keys == set(state_dict.keys()), \"keys of state_dicts are not equal\"\n</code></pre>"},{"location":"api/fusionlib.utils/","title":"fusionlib.utils","text":""},{"location":"api/fusionlib.utils/#fusionlib.utils","title":"<code>fusionlib.utils</code>","text":""},{"location":"api/fusionlib.utils/#fusionlib.utils.args","title":"<code>fusionlib.utils.args</code>","text":""},{"location":"api/fusionlib.utils/#fusionlib.utils.args.verify_str_arg","title":"<code>fusionlib.utils.args.verify_str_arg(value, arg=None, valid_values=None, custom_msg=None, to_lower=False)</code>","text":"<p>check is string argument <code>value</code> with name <code>arg</code> in <code>valid_values</code>, raise <code>ValueError</code> if failed.</p> <p>Examples:</p> <p>if you have a function <code>f</code> accept <code>batch_size</code> as argument, such as:</p> <pre><code>def f(batch_size='half'):\n    verify_str_arg(batch_size, 'batch_size', ['half', 'full'])\n    ...\n</code></pre> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>T</code>)           \u2013            <p><code>str</code> or <code>bytes</code></p> </li> <li> <code>arg</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>. Defaults to None.</p> </li> <li> <code>valid_values</code>               (<code>Iterable[T]</code>, default:                   <code>None</code> )           \u2013            <p>Defaults to <code>None</code>. if this is <code>None</code>, accept any string input.</p> </li> <li> <code>custom_msg</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Defaults to \"Unknown value '{value}' for argument {arg}. Valid values are {{{valid_values}}}.\".</p> </li> <li> <code>to_lower(bool)</code>           \u2013            <p>if <code>True</code>, accept uppercase value.</p> </li> </ul> <p>Raises:     ValueError:</p> <p>Returns:</p> <ul> <li> <code>T</code> (              <code>T</code> )          \u2013            <p>value</p> </li> </ul> Source code in <code>fusionlib/utils/args.py</code> <pre><code>def verify_str_arg(\n    value: T,\n    arg: Optional[str] = None,\n    valid_values: Iterable[T] = None,\n    custom_msg: Optional[str] = None,\n    to_lower: bool = False,\n) -&gt; T:\n    R\"\"\"\n    check is string argument ``value`` with name ``arg`` in `valid_values`, raise `ValueError` if failed.\n\n    Examples:\n\n    if you have a function ``f`` accept `batch_size` as argument, such as:\n\n    ```python\n    def f(batch_size='half'):\n        verify_str_arg(batch_size, 'batch_size', ['half', 'full'])\n        ...\n    ```\n\n    Args:\n        value (T): `str` or `bytes`\n        arg (Optional[str], optional): . Defaults to None.\n        valid_values (Iterable[T], optional): Defaults to `None`. if this is `None`, accept any string input.\n        custom_msg (Optional[str], optional): Defaults to \"Unknown value '{value}' for argument {arg}. Valid values are {{{valid_values}}}.\".\n        to_lower(bool): if `True`, accept uppercase value.\n    Raises:\n        ValueError:\n\n    Returns:\n        T: value\n    \"\"\"\n    if not isinstance(value, (str, bytes)):\n        if arg is None:\n            msg = \"Expected type str, but got type {type}.\"\n        else:\n            msg = \"Expected type str for argument {arg}, but got type {type}.\"\n        msg = msg.format(type=type(value), arg=arg)\n        raise ValueError(msg)\n    else:\n        if valid_values is None:\n            return value\n\n        if to_lower:\n            value = value.lower()\n\n        if value not in valid_values:\n            if custom_msg is not None:\n                msg = custom_msg\n            else:\n                msg = \"Unknown value '{value}' for argument {arg}. Valid values are {{{valid_values}}}.\"\n                msg = msg.format(\n                    value=value, arg=arg, valid_values=_iterable_to_str(valid_values)\n                )\n            raise ValueError(msg)\n\n    return value\n</code></pre>"},{"location":"api/fusionlib.utils/#fusionlib.utils.devices","title":"<code>fusionlib.utils.devices</code>","text":""},{"location":"api/fusionlib.utils/#fusionlib.utils.devices.num_devices","title":"<code>fusionlib.utils.devices.num_devices(devices)</code>","text":"<p>Return the number of devices.</p> <p>Parameters:</p> <ul> <li> <code>devices</code>               (<code>Union[int, List[int], str]</code>)           \u2013            <p><code>devices</code> can be a single int to specify the number of devices, or a list of device ids, e.g. [0, 1, 2, 3]\uff0c or a str of device ids, e.g. \"0,1,2,3\" and \"[0, 1, 2]\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>The number of devices.</p> </li> </ul> Source code in <code>fusionlib/utils/devices.py</code> <pre><code>def num_devices(devices: Union[int, List[int], str]) -&gt; int:\n    \"\"\"\n    Return the number of devices.\n\n    Args:\n        devices: `devices` can be a single int to specify the number of devices, or a list of device ids, e.g. [0, 1, 2, 3]\uff0c or a str of device ids, e.g. \"0,1,2,3\" and \"[0, 1, 2]\".\n\n    Returns:\n        The number of devices.\n    \"\"\"\n    if isinstance(devices, int):\n        return devices\n    elif isinstance(devices, str):\n        return len(devices.split(\",\"))\n    elif isinstance(devices, list):\n        return len(devices)\n    else:\n        raise TypeError(\n            f\"devices must be a single int or a list of ints, but got {type(devices)}\"\n        )\n</code></pre>"},{"location":"api/fusionlib.utils/#fusionlib.utils.path","title":"<code>fusionlib.utils.path</code>","text":""},{"location":"api/fusionlib.utils/#fusionlib.utils.path.listdir_fullpath","title":"<code>fusionlib.utils.path.listdir_fullpath(dir)</code>","text":"<p>list directory <code>dir</code>, return fullpaths</p> <p>Parameters:</p> <ul> <li> <code>dir</code>               (<code>Union[str, Path]</code>)           \u2013            <p>directory name or Path object</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[List[str], List[Path]]</code>           \u2013            <p>Union[List[str], List[Path]]: a list of fullpaths</p> </li> </ul> Source code in <code>fusionlib/utils/path.py</code> <pre><code>def listdir_fullpath(dir: Union[str, Path]) -&gt; Union[List[str], List[Path]]:\n    \"\"\"list directory `dir`, return fullpaths\n\n    Args:\n        dir (Union[str, Path]): directory name or Path object\n\n    Returns:\n        Union[List[str], List[Path]]: a list of fullpaths\n    \"\"\"\n    assert os.path.isdir(dir), \"Argument 'dir' must be a Directory\"\n    if isinstance(dir, str):\n        names = os.listdir(dir)\n        return [os.path.join(dir, name) for name in names]\n    elif isinstance(dir, Path):\n        names = dir.iterdir()\n        return [dir / i for i in names]\n    else:\n        raise TypeError(\"Argument 'dir' must be a string or a Path object\")\n</code></pre>"},{"location":"api/fusionlib.utils/#fusionlib.utils.rich_logging","title":"<code>fusionlib.utils.rich_logging</code>","text":"<p>This module provides utilities for logging with rich output formatting.</p> <p>It defines a function <code>setup_colorlogging</code> that sets up a logging configuration with a <code>RichHandler</code> that outputs log messages with rich formatting to the console. It also defines two functions <code>pprint_code</code> and <code>pprint_yaml</code> that pretty print code snippets in different formats using the <code>Syntax</code> class from the <code>rich.syntax</code> module.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from utils.logging.rich import setup_colorlogging, pprint_yaml\n\n&gt;&gt;&gt; setup_colorlogging()\n&gt;&gt;&gt; pprint_yaml('foo: bar')\n</code></pre>"},{"location":"api/fusionlib.utils/#fusionlib.utils.rich_logging.pprint_json","title":"<code>fusionlib.utils.rich_logging.pprint_json = partial(pprint_code, format='json')</code>  <code>module-attribute</code>","text":"<p>pretty print json code</p>"},{"location":"api/fusionlib.utils/#fusionlib.utils.rich_logging.pprint_yaml","title":"<code>fusionlib.utils.rich_logging.pprint_yaml = partial(pprint_code, format='yaml')</code>  <code>module-attribute</code>","text":"<p>pretty print yaml code</p>"},{"location":"api/fusionlib.utils/#fusionlib.utils.rich_logging.pprint_code","title":"<code>fusionlib.utils.rich_logging.pprint_code(code, format)</code>","text":"<p>pretty print code</p> <p>Parameters:</p> <ul> <li> <code>code</code>               (<code>str</code>)           \u2013            <p>code str</p> </li> <li> <code>format</code>               (<code>str</code>)           \u2013            <p>code format, for example 'yaml'</p> </li> </ul> Source code in <code>fusionlib/utils/rich_logging.py</code> <pre><code>def pprint_code(code: str, format: str):\n    \"\"\"pretty print code\n\n    Args:\n        code (str): code str\n        format (str): code format, for example 'yaml'\n    \"\"\"\n    s = Syntax(code, format)\n    _console.print(s)\n</code></pre>"},{"location":"api/fusionlib.utils/#fusionlib.utils.timer","title":"<code>fusionlib.utils.timer</code>","text":""},{"location":"api/fusionlib.utils/#fusionlib.utils.timer.timer","title":"<code>fusionlib.utils.timer.timer</code>","text":"<p>Usage:</p> <pre><code>```python\n# Usage as a decorator\n@timer\ndef my_func():\n    time.sleep(1)\n\nmy_func()\n\n# Usage as a context manager\nwith timer():\n    time.sleep(1)\n```\n</code></pre> Source code in <code>fusionlib/utils/timer.py</code> <pre><code>class timer:\n    \"\"\"\n    Usage:\n\n        ```python\n        # Usage as a decorator\n        @timer\n        def my_func():\n            time.sleep(1)\n\n        my_func()\n\n        # Usage as a context manager\n        with timer():\n            time.sleep(1)\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        func_or_msg: Optional[Union[Callable, str]] = None,\n        msg: Optional[str] = None,\n        logger: Optional[logging.Logger] = None,\n        loglevel=logging.INFO,\n        human_readable: bool = True,\n    ) -&gt; None:\n        if isinstance(func_or_msg, str):\n            self.func = None\n            self.msg = func_or_msg\n        else:\n            self.func = func_or_msg\n            self.msg = msg\n        self.loglevel = loglevel\n        self.human_readable = human_readable\n\n        if logger is None:\n            self._logger_fn = print\n        else:\n            self._logger_fn = functools.partial(\n                logger.log, level=loglevel, stacklevel=3\n            )\n\n    @property\n    def elapsed_time(self):\n        return time.time() - self.start_time\n\n    def reset(self):\n        self.start_time = time.time()\n\n    def __enter__(self) -&gt; None:\n        self.start_time = time.time()\n        if self.msg is None:\n            if self.func is not None:\n                self.msg = self.func.__name__\n            else:\n                self.msg = \"\"\n        self._logger_fn(f\"[START] {self.msg}\")\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"\n        Calculates the elapsed time and logs it, along with an optional message indicating the end of the code block execution.\n        \"\"\"\n        elapsed_time = self.elapsed_time\n        self._logger_fn(f\"[END] {self.msg} {elapsed_time:.2f}s\")\n\n        # If an exception occurred, log it\n        if exc_type is not None:\n            logger.error(f\"An error occurred: {exc_val}\")\n            # Return True to suppress the exception, False otherwise\n            return False\n\n    def __call__(self, *args, **kwargs):\n        self.__enter__()\n        try:\n            assert self.func is not None, \"func must be set if using as a decorator\"\n            return self.func(*args, **kwargs)\n        except Exception as e:\n            # Pass exception info to __exit__\n            exc_type, exc_val, exc_tb = sys.exc_info()\n            self.__exit__(exc_type, exc_val, exc_tb)\n            raise  # re-raise the exception\n        finally:\n            # No exception, so pass None as all three arguments\n            self.__exit__(None, None, None)\n</code></pre>"},{"location":"api/fusionlib.utils/#fusionlib.utils.timer.timer.__exit__","title":"<code>fusionlib.utils.timer.timer.__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Calculates the elapsed time and logs it, along with an optional message indicating the end of the code block execution.</p> Source code in <code>fusionlib/utils/timer.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"\n    Calculates the elapsed time and logs it, along with an optional message indicating the end of the code block execution.\n    \"\"\"\n    elapsed_time = self.elapsed_time\n    self._logger_fn(f\"[END] {self.msg} {elapsed_time:.2f}s\")\n\n    # If an exception occurred, log it\n    if exc_type is not None:\n        logger.error(f\"An error occurred: {exc_val}\")\n        # Return True to suppress the exception, False otherwise\n        return False\n</code></pre>"},{"location":"api/fusionlib.utils/#fusionlib.utils.torch","title":"<code>fusionlib.utils.torch</code>","text":""},{"location":"api/fusionlib.utils/#fusionlib.utils.torch.parameters","title":"<code>fusionlib.utils.torch.parameters</code>","text":""},{"location":"api/fusionlib.utils/#fusionlib.utils.torch.parameters.check_parameters_all_equal","title":"<code>fusionlib.utils.torch.parameters.check_parameters_all_equal(list_of_param_names)</code>","text":"<p>Checks if all models have the same parameters.</p> <p>This function takes a list of parameter names or state dictionaries from different models. It checks if all models have the same parameters by comparing the parameter names. If any model has different parameters, it raises a ValueError with the differing parameters.</p> <p>Parameters:</p> <ul> <li> <code>list_of_param_names</code>               (<code>List[Union[StateDict, List[str]]]</code>)           \u2013            <p>A list of parameter names or state dictionaries.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If any model has different parameters.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>None</p> </li> </ul> Source code in <code>fusionlib/utils/torch/parameters.py</code> <pre><code>def check_parameters_all_equal(\n    list_of_param_names: List[Union[_StateDict, nn.Module, List[str]]]\n) -&gt; None:\n    \"\"\"\n    Checks if all models have the same parameters.\n\n    This function takes a list of parameter names or state dictionaries from different models.\n    It checks if all models have the same parameters by comparing the parameter names.\n    If any model has different parameters, it raises a ValueError with the differing parameters.\n\n    Args:\n        list_of_param_names (List[Union[StateDict, List[str]]]): A list of parameter names or state dictionaries.\n\n    Raises:\n        ValueError: If any model has different parameters.\n\n    Returns:\n        None\n    \"\"\"\n    if isinstance(list_of_param_names[0], Mapping):\n        list_of_param_names = [list(i.keys()) for i in list_of_param_names]\n    elif isinstance(list_of_param_names[0], nn.Module):\n        list_of_param_names = [list(i.state_dict().keys()) for i in list_of_param_names]\n    else:\n        parameter_names = set(list_of_param_names[0])\n\n        if len(list_of_param_names) &gt;= 2:\n            # raise ValueError(\"Number of models is less than 2.\")\n            for names in list_of_param_names[1:]:\n                current_parameterNames = set(names)\n                if current_parameterNames != parameter_names:\n                    raise ValueError(\n                        \"Differing parameter names in models. \"\n                        f\"The different parameters are {parameter_names.symmetric_difference(current_parameterNames)}\"\n                    )\n</code></pre>"},{"location":"api/fusionlib.utils/#fusionlib.utils.torch.parameters.count_parameters","title":"<code>fusionlib.utils.torch.parameters.count_parameters(module)</code>","text":"<p>Counts the number of trainable and total parameters in a PyTorch model.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The PyTorch model for which to count parameters.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple</code>          \u2013            <p>A tuple containing the number of trainable parameters and the total number of parameters.</p> </li> </ul> <p>Examples:</p> <pre><code>```python\n# Count the parameters\ntrainable_params, all_params = count_parameters(model)\n```\n</code></pre> Source code in <code>fusionlib/utils/torch/parameters.py</code> <pre><code>@torch.no_grad()\ndef count_parameters(module: nn.Module):\n    \"\"\"\n    Counts the number of trainable and total parameters in a PyTorch model.\n\n    Args:\n        model (nn.Module): The PyTorch model for which to count parameters.\n\n    Returns:\n        tuple: A tuple containing the number of trainable parameters and the total number of parameters.\n\n    Examples:\n\n        ```python\n        # Count the parameters\n        trainable_params, all_params = count_parameters(model)\n        ```\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for name, param in module.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    return trainable_params, all_param\n</code></pre>"},{"location":"api/fusionlib.utils/#fusionlib.utils.torch.parameters.print_parameters","title":"<code>fusionlib.utils.torch.parameters.print_parameters(module, human_readable=True)</code>","text":"<p>Prints the number of trainable and total parameters in a PyTorch model.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>Module</code>)           \u2013            <p>The PyTorch model for which to print parameters.</p> </li> <li> <code>human_readable</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, the parameter counts are converted to a human-readable format (e.g., '1.5M' instead of '1500000'). Defaults to True.</p> </li> </ul> Prints <p>The number of trainable parameters, the total number of parameters, and the percentage of trainable parameters in the model.</p> Source code in <code>fusionlib/utils/torch/parameters.py</code> <pre><code>def print_parameters(\n    module: nn.Module,\n    human_readable: bool = True,\n):\n    \"\"\"\n    Prints the number of trainable and total parameters in a PyTorch model.\n\n    Args:\n        module (nn.Module): The PyTorch model for which to print parameters.\n        human_readable (bool, optional): If True, the parameter counts are converted to a human-readable format (e.g., '1.5M' instead of '1500000'). Defaults to True.\n\n    Prints:\n        The number of trainable parameters, the total number of parameters, and the percentage of trainable parameters in the model.\n    \"\"\"\n    trainable_params, all_param = count_parameters(module)\n    if human_readable:\n        trainable_params = human_readable(trainable_params)\n        all_param = human_readable(all_param)\n\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.4f}\"\n    )\n</code></pre>"},{"location":"api/fusionlib.utils/#fusionlib.utils.torch.state_dict","title":"<code>fusionlib.utils.torch.state_dict</code>","text":""},{"location":"api/fusionlib.utils/#fusionlib.utils.torch.state_dict.StateDict","title":"<code>fusionlib.utils.torch.state_dict.StateDict</code>","text":"<p>A wrapper class for PyTorch state dictionaries.</p> <p>This class allows arithmetic operations to be performed directly on state dictionaries. It can be initialized with a PyTorch module, a state dictionary, or another StateDict instance.</p> <p>Parameters:</p> <ul> <li> <code>state_dict</code>               (<code>Union[_StateDict, StateDict, Module]</code>)           \u2013            <p>The initial state dictionary. This can be a PyTorch module, a state dictionary, or another StateDict instance.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the state_dict argument is not a PyTorch module, a state dictionary, or a StateDict instance.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch.nn as nn\n&gt;&gt;&gt; model = nn.Linear(10, 10)\n&gt;&gt;&gt; state_dict = StateDict(model)\n&gt;&gt;&gt; print(state_dict['weight'])  # Access state dictionary items directly\n&gt;&gt;&gt; state_dict2 = state_dict + 0.1  # Add a scalar to all parameters\n&gt;&gt;&gt; state_dict3 = state_dict * 0.9  # Multiply all parameters by a scalar\n</code></pre> Source code in <code>fusionlib/utils/torch/state_dict.py</code> <pre><code>class StateDict:\n    \"\"\"\n    A wrapper class for PyTorch state dictionaries.\n\n    This class allows arithmetic operations to be performed directly on state dictionaries. It can be initialized with a PyTorch module, a state dictionary, or another StateDict instance.\n\n    Args:\n        state_dict (Union[_StateDict, \"StateDict\", nn.Module]): The initial state dictionary. This can be a PyTorch module, a state dictionary, or another StateDict instance.\n\n    Raises:\n        TypeError: If the state_dict argument is not a PyTorch module, a state dictionary, or a StateDict instance.\n\n    Examples:\n        &gt;&gt;&gt; import torch.nn as nn\n        &gt;&gt;&gt; model = nn.Linear(10, 10)\n        &gt;&gt;&gt; state_dict = StateDict(model)\n        &gt;&gt;&gt; print(state_dict['weight'])  # Access state dictionary items directly\n        &gt;&gt;&gt; state_dict2 = state_dict + 0.1  # Add a scalar to all parameters\n        &gt;&gt;&gt; state_dict3 = state_dict * 0.9  # Multiply all parameters by a scalar\n    \"\"\"\n\n    def __init__(self, state_dict: Union[_StateDict, \"StateDict\", nn.Module]):\n        if isinstance(state_dict, nn.Module):\n            state_dict = state_dict.state_dict()\n        elif isinstance(state_dict, Mapping):\n            self.state_dict = state_dict\n        elif isinstance(state_dict, StateDict):\n            self.state_dict = state_dict.state_dict\n        else:\n            raise TypeError(\n                f\"unsupported type for state_dict: '{type(state_dict).__name__}'\"\n            )\n\n    def __add__(self, other: Union[\"StateDict\", _StateDict, Number]) -&gt; \"StateDict\":\n        if isinstance(other, (StateDict, Mapping)):\n            return StateDict(state_dict_add(self, other))\n        elif isinstance(other, Number):\n            return StateDict(state_dict_add_scalar(self, other))\n        else:\n            raise TypeError(\n                f\"unsupported operand type(s) for +: 'StateDict' and '{type(other).__name__}'\"\n            )\n\n    def __iadd__(self, other: Number) -&gt; \"StateDict\":\n        if isinstance(other, Number):\n            for key in self.state_dict:\n                self.state_dict[key] += other\n        else:\n            raise TypeError(\n                f\"unsupported operand type(s) for +: 'StateDict' and '{type(other).__name__}'\"\n            )\n\n    def __sub__(self, other: Union[\"StateDict\", float]) -&gt; \"StateDict\":\n        if isinstance(other, (StateDict, Mapping)):\n            return StateDict(state_dict_sub(self, other))\n        elif isinstance(other, Number):\n            return StateDict(state_dict_add_scalar(self, -other))\n        else:\n            raise TypeError(\n                f\"unsupported operand type(s) for -: 'StateDict' and '{type(other).__name__}'\"\n            )\n\n    def __isub__(self, other: Number) -&gt; \"StateDict\":\n        if isinstance(other, Number):\n            for key in self.state_dict:\n                self.state_dict[key] -= other\n        else:\n            raise TypeError(\n                f\"unsupported operand type(s) for -: 'StateDict' and '{type(other).__name__}'\"\n            )\n\n    def __mul__(self, other: Number) -&gt; \"StateDict\":\n        if isinstance(other, Number):\n            return StateDict(state_dict_mul(self, other))\n        else:\n            raise TypeError(\n                f\"unsupported operand type(s) for *: 'StateDict' and '{type(other).__name__}'\"\n            )\n\n    def __getattr__(self, name: str) -&gt; Any:\n        return getattr(self.state_dict, name)\n\n    def to_vector(self) -&gt; torch.Tensor:\n        \"\"\"\n        Convert the state dictionary to a vector.\n\n        Returns:\n            torch.Tensor: The state dictionary as a vector.\n        \"\"\"\n        return torch.nn.utils.parameters_to_vector(self.state_dict.values())\n\n    def from_vector(self, vector: torch.Tensor):\n        \"\"\"\n        Convert a vector to a state dictionary.\n\n        Args:\n            vector (torch.Tensor): The vector to convert to a state dictionary.\n        \"\"\"\n        torch.nn.utils.vector_to_parameters(vector, self.state_dict.values())\n        return self\n</code></pre>"},{"location":"api/fusionlib.utils/#fusionlib.utils.torch.state_dict.StateDict.from_vector","title":"<code>fusionlib.utils.torch.state_dict.StateDict.from_vector(vector)</code>","text":"<p>Convert a vector to a state dictionary.</p> <p>Parameters:</p> <ul> <li> <code>vector</code>               (<code>Tensor</code>)           \u2013            <p>The vector to convert to a state dictionary.</p> </li> </ul> Source code in <code>fusionlib/utils/torch/state_dict.py</code> <pre><code>def from_vector(self, vector: torch.Tensor):\n    \"\"\"\n    Convert a vector to a state dictionary.\n\n    Args:\n        vector (torch.Tensor): The vector to convert to a state dictionary.\n    \"\"\"\n    torch.nn.utils.vector_to_parameters(vector, self.state_dict.values())\n    return self\n</code></pre>"},{"location":"api/fusionlib.utils/#fusionlib.utils.torch.state_dict.StateDict.to_vector","title":"<code>fusionlib.utils.torch.state_dict.StateDict.to_vector()</code>","text":"<p>Convert the state dictionary to a vector.</p> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>torch.Tensor: The state dictionary as a vector.</p> </li> </ul> Source code in <code>fusionlib/utils/torch/state_dict.py</code> <pre><code>def to_vector(self) -&gt; torch.Tensor:\n    \"\"\"\n    Convert the state dictionary to a vector.\n\n    Returns:\n        torch.Tensor: The state dictionary as a vector.\n    \"\"\"\n    return torch.nn.utils.parameters_to_vector(self.state_dict.values())\n</code></pre>"},{"location":"api/fusionlib.utils/#fusionlib.utils.torch.state_dict_arithmetic","title":"<code>fusionlib.utils.torch.state_dict_arithmetic</code>","text":""},{"location":"api/fusionlib.utils/#fusionlib.utils.torch.state_dict_arithmetic.num_params_of_state_dict","title":"<code>fusionlib.utils.torch.state_dict_arithmetic.num_params_of_state_dict(state_dict)</code>","text":"<p>Returns the number of parameters in a state dict.</p> <p>Parameters:</p> <ul> <li> <code>state_dict</code>               (<code>Dict[str, Tensor]</code>)           \u2013            <p>The state dict to count the number of parameters in.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>          \u2013            <p>The number of parameters in the state dict.</p> </li> </ul> Source code in <code>fusionlib/utils/torch/state_dict_arithmetic.py</code> <pre><code>def num_params_of_state_dict(state_dict: _StateDict):\n    \"\"\"\n    Returns the number of parameters in a state dict.\n\n    Args:\n        state_dict (Dict[str, Tensor]): The state dict to count the number of parameters in.\n\n    Returns:\n        int: The number of parameters in the state dict.\n    \"\"\"\n    return sum([state_dict[key].numel() for key in state_dict])\n</code></pre>"},{"location":"api/fusionlib.utils/#fusionlib.utils.torch.state_dict_arithmetic.state_dict_add","title":"<code>fusionlib.utils.torch.state_dict_arithmetic.state_dict_add(a, b, strict=True, device=None)</code>","text":"<p>Returns the sum of two state dicts.</p> <p>Parameters:</p> <ul> <li> <code>a</code>               (<code>Dict</code>)           \u2013            <p>The first state dict.</p> </li> <li> <code>b</code>               (<code>Dict</code>)           \u2013            <p>The second state dict.</p> </li> <li> <code>strict</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to check if the keys of the two state dicts are the same.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code>          \u2013            <p>The sum of the two state dicts.</p> </li> </ul> Source code in <code>fusionlib/utils/torch/state_dict_arithmetic.py</code> <pre><code>def state_dict_add(a: _StateDict, b: _StateDict, strict: bool = True, device=None):\n    \"\"\"\n    Returns the sum of two state dicts.\n\n    Args:\n        a (Dict): The first state dict.\n        b (Dict): The second state dict.\n        strict (bool): Whether to check if the keys of the two state dicts are the same.\n\n    Returns:\n        Dict: The sum of the two state dicts.\n    \"\"\"\n    ans = OrderedDict()\n    if strict:\n        check_parameters_all_equal([a, b])\n        for key in a:\n            ans[key] = a[key] + b[key]\n    else:\n        for key in a:\n            if key in b:\n                ans[key] = a[key] + b[key]\n    if device is not None:\n        ans = to_device(ans, device)\n    return ans\n</code></pre>"},{"location":"api/fusionlib.utils/#fusionlib.utils.torch.state_dict_arithmetic.state_dict_avg","title":"<code>fusionlib.utils.torch.state_dict_arithmetic.state_dict_avg(state_dicts)</code>","text":"<p>Returns the average of a list of state dicts.</p> <p>Parameters:</p> <ul> <li> <code>state_dicts</code>               (<code>List[Dict[str, Tensor]]</code>)           \u2013            <p>The list of state dicts to average.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code>          \u2013            <p>The average of the state dicts.</p> </li> </ul> Source code in <code>fusionlib/utils/torch/state_dict_arithmetic.py</code> <pre><code>def state_dict_avg(state_dicts: List[_StateDict]):\n    \"\"\"\n    Returns the average of a list of state dicts.\n\n    Args:\n        state_dicts (List[Dict[str, Tensor]]): The list of state dicts to average.\n\n    Returns:\n        Dict: The average of the state dicts.\n    \"\"\"\n    assert len(state_dicts) &gt; 0, \"The number of state_dicts must be greater than 0\"\n    assert all(\n        [len(state_dicts[0]) == len(state_dict) for state_dict in state_dicts]\n    ), \"All state_dicts must have the same number of keys\"\n\n    num_state_dicts = len(state_dicts)\n    avg_state_dict = OrderedDict()\n    for key in state_dicts[0]:\n        avg_state_dict[key] = torch.zeros_like(state_dicts[0][key])\n        for state_dict in state_dicts:\n            avg_state_dict[key] += state_dict[key]\n        avg_state_dict[key] /= num_state_dicts\n    return avg_state_dict\n</code></pre>"},{"location":"api/fusionlib.utils/#fusionlib.utils.torch.state_dict_arithmetic.state_dict_flatten","title":"<code>fusionlib.utils.torch.state_dict_arithmetic.state_dict_flatten(state_dict)</code>","text":"<p>Flattens a state dict.</p> <p>Parameters:</p> <ul> <li> <code>state_dict</code>               (<code>Dict[str, Tensor]</code>)           \u2013            <p>The state dict to be flattened.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>          \u2013            <p>The flattened state dict.</p> </li> </ul> Source code in <code>fusionlib/utils/torch/state_dict_arithmetic.py</code> <pre><code>def state_dict_flatten(state_dict: Dict[str, Tensor]):\n    \"\"\"\n    Flattens a state dict.\n\n    Args:\n        state_dict (Dict[str, Tensor]): The state dict to be flattened.\n\n    Returns:\n        Tensor: The flattened state dict.\n    \"\"\"\n    flattened_state_dict = []\n    for key in state_dict:\n        flattened_state_dict.append(state_dict[key].flatten())\n    return torch.cat(flattened_state_dict)\n</code></pre>"},{"location":"api/fusionlib.utils/#fusionlib.utils.torch.state_dict_arithmetic.state_dict_interpolation","title":"<code>fusionlib.utils.torch.state_dict_arithmetic.state_dict_interpolation(state_dicts, scalars)</code>","text":"<p>Interpolates between a list of state dicts using a list of scalars.</p> <p>Parameters:</p> <ul> <li> <code>state_dicts</code>               (<code>List[Dict[str, Tensor]]</code>)           \u2013            <p>The list of state dicts to interpolate between.</p> </li> <li> <code>scalars</code>               (<code>List[float]</code>)           \u2013            <p>The list of scalars to use for interpolation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code>          \u2013            <p>The interpolated state dict.</p> </li> </ul> Source code in <code>fusionlib/utils/torch/state_dict_arithmetic.py</code> <pre><code>def state_dict_interpolation(\n    state_dicts: List[Dict[str, Tensor]], scalars: List[float]\n):\n    \"\"\"\n    Interpolates between a list of state dicts using a list of scalars.\n\n    Args:\n        state_dicts (List[Dict[str, Tensor]]): The list of state dicts to interpolate between.\n        scalars (List[float]): The list of scalars to use for interpolation.\n\n    Returns:\n        Dict: The interpolated state dict.\n    \"\"\"\n    assert len(state_dicts) == len(\n        scalars\n    ), \"The number of state_dicts and scalars must be the same\"\n    assert len(state_dicts) &gt; 0, \"The number of state_dicts must be greater than 0\"\n    assert all(\n        [len(state_dicts[0]) == len(state_dict) for state_dict in state_dicts]\n    ), \"All state_dicts must have the same number of keys\"\n\n    interpolated_state_dict = {}\n    for key in state_dicts[0]:\n        interpolated_state_dict[key] = torch.zeros_like(state_dicts[0][key])\n        for state_dict, scalar in zip(state_dicts, scalars):\n            interpolated_state_dict[key] += scalar * state_dict[key]\n    return interpolated_state_dict\n</code></pre>"},{"location":"api/fusionlib.utils/#fusionlib.utils.torch.state_dict_arithmetic.state_dict_mul","title":"<code>fusionlib.utils.torch.state_dict_arithmetic.state_dict_mul(state_dict, scalar)</code>","text":"<p>Returns the product of a state dict and a scalar.</p> <p>Parameters:</p> <ul> <li> <code>state_dict</code>               (<code>Dict</code>)           \u2013            <p>The state dict to be multiplied.</p> </li> <li> <code>scalar</code>               (<code>float</code>)           \u2013            <p>The scalar to multiply the state dict with.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code>          \u2013            <p>The product of the state dict and the scalar.</p> </li> </ul> Source code in <code>fusionlib/utils/torch/state_dict_arithmetic.py</code> <pre><code>def state_dict_mul(state_dict: _StateDict, scalar: float):\n    \"\"\"\n    Returns the product of a state dict and a scalar.\n\n    Args:\n        state_dict (Dict): The state dict to be multiplied.\n        scalar (float): The scalar to multiply the state dict with.\n\n    Returns:\n        Dict: The product of the state dict and the scalar.\n    \"\"\"\n    diff = OrderedDict()\n    for k in state_dict:\n        diff[k] = scalar * state_dict[k]\n    return diff\n</code></pre>"},{"location":"api/fusionlib.utils/#fusionlib.utils.torch.state_dict_arithmetic.state_dict_power","title":"<code>fusionlib.utils.torch.state_dict_arithmetic.state_dict_power(state_dict, p)</code>","text":"<p>Returns the power of a state dict.</p> <p>Parameters:</p> <ul> <li> <code>state_dict</code>               (<code>Dict[str, Tensor]</code>)           \u2013            <p>The state dict to be powered.</p> </li> <li> <code>p</code>               (<code>float</code>)           \u2013            <p>The power to raise the state dict to.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Dict[str, Tensor]: The powered state dict.</p> </li> </ul> Source code in <code>fusionlib/utils/torch/state_dict_arithmetic.py</code> <pre><code>def state_dict_power(state_dict: Dict[str, Tensor], p: float):\n    \"\"\"\n    Returns the power of a state dict.\n\n    Args:\n        state_dict (Dict[str, Tensor]): The state dict to be powered.\n        p (float): The power to raise the state dict to.\n\n    Returns:\n        Dict[str, Tensor]: The powered state dict.\n    \"\"\"\n    powered_state_dict = {}\n    for key in state_dict:\n        powered_state_dict[key] = state_dict[key] ** p\n    return powered_state_dict\n</code></pre>"},{"location":"api/fusionlib.utils/#fusionlib.utils.torch.state_dict_arithmetic.state_dict_sub","title":"<code>fusionlib.utils.torch.state_dict_arithmetic.state_dict_sub(a, b, strict=True, device=None)</code>","text":"<p>Returns the difference between two state dicts <code>a-b</code>.</p> <p>Parameters:</p> <ul> <li> <code>a</code>               (<code>_StateDict</code>)           \u2013            <p>The first state dict.</p> </li> <li> <code>b</code>               (<code>_StateDict</code>)           \u2013            <p>The second state dict.</p> </li> <li> <code>strict</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to check if the keys of the two state dicts are the same.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>_StateDict</code>          \u2013            <p>The difference between the two state dicts.</p> </li> </ul> Source code in <code>fusionlib/utils/torch/state_dict_arithmetic.py</code> <pre><code>def state_dict_sub(a: _StateDict, b: _StateDict, strict: bool = True, device=None):\n    \"\"\"\n    Returns the difference between two state dicts `a-b`.\n\n    Args:\n        a (_StateDict): The first state dict.\n        b (_StateDict): The second state dict.\n        strict (bool): Whether to check if the keys of the two state dicts are the same.\n\n    Returns:\n        _StateDict: The difference between the two state dicts.\n    \"\"\"\n    if strict:\n        assert set(a.keys()) == set(b.keys())\n\n    diff = OrderedDict()\n    for k in a:\n        if k in b:\n            diff[k] = a[k] - b[k]\n            if device is not None:\n                diff[k] = diff[k].to(device, non_blocking=True)\n    return diff\n</code></pre>"},{"location":"api/fusionlib.utils/#fusionlib.utils.torch.state_dict_arithmetic.state_dict_sum","title":"<code>fusionlib.utils.torch.state_dict_arithmetic.state_dict_sum(state_dicts)</code>","text":"<p>Returns the sum of a list of state dicts.</p> <p>Parameters:</p> <ul> <li> <code>state_dicts</code>               (<code>List[Dict[str, Tensor]]</code>)           \u2013            <p>The list of state dicts to sum.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code>          \u2013            <p>The sum of the state dicts.</p> </li> </ul> Source code in <code>fusionlib/utils/torch/state_dict_arithmetic.py</code> <pre><code>def state_dict_sum(state_dicts: List[_StateDict]):\n    \"\"\"\n    Returns the sum of a list of state dicts.\n\n    Args:\n        state_dicts (List[Dict[str, Tensor]]): The list of state dicts to sum.\n\n    Returns:\n        Dict: The sum of the state dicts.\n    \"\"\"\n    assert len(state_dicts) &gt; 0, \"The number of state_dicts must be greater than 0\"\n    assert all(\n        [len(state_dicts[0]) == len(state_dict) for state_dict in state_dicts]\n    ), \"All state_dicts must have the same number of keys\"\n\n    sum_state_dict = OrderedDict()\n    for key in state_dicts[0]:\n        sum_state_dict[key] = torch.zeros_like(state_dicts[0][key])\n        for state_dict in state_dicts:\n            sum_state_dict[key] += state_dict[key]\n    return sum_state_dict\n</code></pre>"},{"location":"api/fusionlib.utils/#fusionlib.utils.torch.state_dict_arithmetic.state_dict_weighted_sum","title":"<code>fusionlib.utils.torch.state_dict_arithmetic.state_dict_weighted_sum(state_dicts, weights, device=None)</code>","text":"<p>Returns the weighted sum of a list of state dicts.</p> <p>Parameters:</p> <ul> <li> <code>state_dicts</code>               (<code>List[Dict[str, Tensor]]</code>)           \u2013            <p>The list of state dicts to interpolate between.</p> </li> <li> <code>weights</code>               (<code>List[float]</code>)           \u2013            <p>The list of weights to use for the weighted sum.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code>          \u2013            <p>The weighted sum of the state dicts.</p> </li> </ul> Source code in <code>fusionlib/utils/torch/state_dict_arithmetic.py</code> <pre><code>def state_dict_weighted_sum(\n    state_dicts: List[Dict[str, Tensor]], weights: List[float], device=None\n):\n    \"\"\"\n    Returns the weighted sum of a list of state dicts.\n\n    Args:\n        state_dicts (List[Dict[str, Tensor]]): The list of state dicts to interpolate between.\n        weights (List[float]): The list of weights to use for the weighted sum.\n\n    Returns:\n        Dict: The weighted sum of the state dicts.\n    \"\"\"\n    assert len(state_dicts) == len(\n        weights\n    ), \"The number of state_dicts and weights must be the same\"\n    assert len(state_dicts) &gt; 0, \"The number of state_dicts must be greater than 0\"\n    assert all(\n        [len(state_dicts[0]) == len(state_dict) for state_dict in state_dicts]\n    ), \"All state_dicts must have the same number of keys\"\n\n    weighted_sum_state_dict = {}\n    for key in state_dicts[0]:\n        weighted_sum_state_dict[key] = torch.zeros_like(state_dicts[0][key])\n        for state_dict, weight in zip(state_dicts, weights):\n            if device is None:\n                weighted_sum_state_dict[key] += weight * state_dict[key]\n            else:\n                # NOTE: if weight is a tensor, state_dict and weight must be on the same device\n                weighted_sum_state_dict[key] += (weight * state_dict[key]).to(\n                    device, non_blocking=True\n                )\n    return weighted_sum_state_dict\n    return weighted_sum_state_dict\n</code></pre>"},{"location":"api/fusionlib.utils/#fusionlib.utils.torch.state_dict_arithmetic.state_dicts_check_keys","title":"<code>fusionlib.utils.torch.state_dict_arithmetic.state_dicts_check_keys(state_dicts)</code>","text":"<p>Checks that the state dictionaries have the same keys.</p> <p>Parameters:</p> <ul> <li> <code>state_dicts</code>               (<code>List[Dict[str, Tensor]]</code>)           \u2013            <p>A list of dictionaries containing the state of PyTorch models.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the state dictionaries have different keys.</p> </li> </ul> Source code in <code>fusionlib/utils/torch/state_dict_arithmetic.py</code> <pre><code>def state_dicts_check_keys(state_dicts: List[_StateDict]):\n    \"\"\"\n    Checks that the state dictionaries have the same keys.\n\n    Args:\n        state_dicts (List[Dict[str, Tensor]]): A list of dictionaries containing the state of PyTorch models.\n\n    Raises:\n        ValueError: If the state dictionaries have different keys.\n    \"\"\"\n    # Get the keys of the first state dictionary in the list\n    keys = set(state_dicts[0].keys())\n    # Check that all the state dictionaries have the same keys\n    for state_dict in state_dicts:\n        assert keys == set(state_dict.keys()), \"keys of state_dicts are not equal\"\n</code></pre>"},{"location":"merging/simple_averaging/","title":"Simple Averaging","text":"<p>Averaging models fine-tuned from a common pre-trained model is a simple yet effective way to improve performance. </p>"},{"location":"merging/simple_averaging/#fusionlib.merge.average.simple_average","title":"<code>fusionlib.merge.average.simple_average(modules)</code>","text":"<p>Averages the parameters of a list of PyTorch modules or state dictionaries.</p> <p>This function takes a list of PyTorch modules or state dictionaries and returns a new module with the averaged parameters, or a new state dictionary with the averaged parameters.</p> <p>Parameters:</p> <ul> <li> <code>modules</code>               (<code>List[Union[Module, _StateDict]]</code>)           \u2013            <p>A list of PyTorch modules or state dictionaries.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>module_or_state_dict</code> (              <code>Union[Module, _StateDict]</code> )          \u2013            <p>A new PyTorch module with the averaged parameters, or a new state dictionary with the averaged parameters.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch.nn as nn\n&gt;&gt;&gt; model1 = nn.Linear(10, 10)\n&gt;&gt;&gt; model2 = nn.Linear(10, 10)\n&gt;&gt;&gt; averaged_model = simple_averageing([model1, model2])\n</code></pre> <pre><code>&gt;&gt;&gt; state_dict1 = model1.state_dict()\n&gt;&gt;&gt; state_dict2 = model2.state_dict()\n&gt;&gt;&gt; averaged_state_dict = simple_averageing([state_dict1, state_dict2])\n</code></pre> Source code in <code>fusionlib/merge/average.py</code> <pre><code>def simple_average(modules: List[Union[nn.Module, _StateDict]]):\n    \"\"\"\n    Averages the parameters of a list of PyTorch modules or state dictionaries.\n\n    This function takes a list of PyTorch modules or state dictionaries and returns a new module with the averaged parameters, or a new state dictionary with the averaged parameters.\n\n    Args:\n        modules (List[Union[nn.Module, _StateDict]]): A list of PyTorch modules or state dictionaries.\n\n    Returns:\n        module_or_state_dict (Union[nn.Module, _StateDict]): A new PyTorch module with the averaged parameters, or a new state dictionary with the averaged parameters.\n\n    Examples:\n        &gt;&gt;&gt; import torch.nn as nn\n        &gt;&gt;&gt; model1 = nn.Linear(10, 10)\n        &gt;&gt;&gt; model2 = nn.Linear(10, 10)\n        &gt;&gt;&gt; averaged_model = simple_averageing([model1, model2])\n\n        &gt;&gt;&gt; state_dict1 = model1.state_dict()\n        &gt;&gt;&gt; state_dict2 = model2.state_dict()\n        &gt;&gt;&gt; averaged_state_dict = simple_averageing([state_dict1, state_dict2])\n    \"\"\"\n    if isinstance(modules[0], nn.Module):\n        new_module = deepcopy(modules[0])\n        state_dict = state_dict_avg([module.state_dict() for module in modules])\n        new_module.load_state_dict(state_dict)\n        return new_module\n    elif isinstance(modules[0], Mapping):\n        return state_dict_avg(modules)\n</code></pre>"},{"location":"merging/task_arithmetic/","title":"Task Arithmetic","text":"\\[\\theta = \\theta_0 + \\lambda \\sum_i \\tau_i\\]"},{"location":"merging/task_arithmetic/#fusionlib.merge.task_arithmetic","title":"<code>fusionlib.merge.task_arithmetic</code>","text":""},{"location":"merging/task_arithmetic/#fusionlib.merge.task_arithmetic.task_arithmetic_merge_modules","title":"<code>fusionlib.merge.task_arithmetic.task_arithmetic_merge_modules(pretrained_model, finetuned_models, scaling_coef)</code>","text":"<p>Merges a pretrained model with a list of fine-tuned models using task arithmetic.</p> <p>Parameters:</p> <ul> <li> <code>pretrained_model</code>               (<code>Module</code>)           \u2013            <p>The pretrained model.</p> </li> <li> <code>finetuned_models</code>               (<code>List[Module]</code>)           \u2013            <p>A list of fine-tuned models.</p> </li> <li> <code>scaling_coef</code>               (<code>float</code>)           \u2013            <p>The scaling coefficient to apply to the sum of the fine-tuned state dictionaries.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>nn.Module: The merged model.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pretrained_model = torch.nn.Linear(10, 10)\n&gt;&gt;&gt; finetuned_models = [torch.nn.Linear(10, 10), torch.nn.Linear(10, 10)]\n&gt;&gt;&gt; scaling_coef = 0.1\n&gt;&gt;&gt; new_model = task_arithmetic_merge_modules(pretrained_model, finetuned_models, scaling_coef)\n</code></pre> Source code in <code>fusionlib/merge/task_arithmetic.py</code> <pre><code>@torch.no_grad()\ndef task_arithmetic_merge_modules(\n    pretrained_model: nn.Module, finetuned_models: List[nn.Module], scaling_coef: float\n):\n    \"\"\"\n    Merges a pretrained model with a list of fine-tuned models using task arithmetic.\n\n    Args:\n        pretrained_model (nn.Module): The pretrained model.\n        finetuned_models (List[nn.Module]): A list of fine-tuned models.\n        scaling_coef (float): The scaling coefficient to apply to the sum of the fine-tuned state dictionaries.\n\n    Returns:\n        nn.Module: The merged model.\n\n    Examples:\n        &gt;&gt;&gt; pretrained_model = torch.nn.Linear(10, 10)\n        &gt;&gt;&gt; finetuned_models = [torch.nn.Linear(10, 10), torch.nn.Linear(10, 10)]\n        &gt;&gt;&gt; scaling_coef = 0.1\n        &gt;&gt;&gt; new_model = task_arithmetic_merge_modules(pretrained_model, finetuned_models, scaling_coef)\n    \"\"\"\n    pretrained_state_dict = pretrained_model.state_dict()\n    finetuned_state_dicts = [model.state_dict() for model in finetuned_models]\n    new_state_dict = task_arithmetic_merge_state_dicts(\n        pretrained_state_dict, finetuned_state_dicts, scaling_coef\n    )\n    model = deepcopy(pretrained_model)\n    model.load_state_dict(new_state_dict)\n    return model\n</code></pre>"},{"location":"merging/task_arithmetic/#fusionlib.merge.task_arithmetic.task_arithmetic_merge_state_dicts","title":"<code>fusionlib.merge.task_arithmetic.task_arithmetic_merge_state_dicts(pretrained_state_dict, finetuned_state_dicts, scaling_coef)</code>","text":"<p>Examples:</p> <pre><code>&gt;&gt;&gt; pretrained_state_dict = model.state_dict()\n&gt;&gt;&gt; finetuned_state_dicts = [model1.state_dict(), model2.state_dict()]\n&gt;&gt;&gt; scaling_coef = 0.1\n&gt;&gt;&gt; new_state_dict = task_arithmetic_merge_state_dicts(pretrained_state_dict, finetuned_state_dicts, scaling_coef)\n</code></pre> Source code in <code>fusionlib/merge/task_arithmetic.py</code> <pre><code>def task_arithmetic_merge_state_dicts(\n    pretrained_state_dict: _StateDict,\n    finetuned_state_dicts: List[_StateDict],\n    scaling_coef: float,\n):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; pretrained_state_dict = model.state_dict()\n        &gt;&gt;&gt; finetuned_state_dicts = [model1.state_dict(), model2.state_dict()]\n        &gt;&gt;&gt; scaling_coef = 0.1\n        &gt;&gt;&gt; new_state_dict = task_arithmetic_merge_state_dicts(pretrained_state_dict, finetuned_state_dicts, scaling_coef)\n    \"\"\"\n    task_vectors = [\n        state_dict_sub(finetuned_state_dict, pretrained_state_dict)\n        for finetuned_state_dict in finetuned_state_dicts\n    ]\n    task_vector = state_dict_sum(task_vectors)\n    task_vector = state_dict_mul(task_vector, scaling_coef)\n    new_state_dict = state_dict_add(pretrained_state_dict, task_vector)\n    return new_state_dict\n</code></pre>"}]}